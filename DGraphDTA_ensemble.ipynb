{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simplified_Compounds_Images_GraphComb_CNN_DGraphDTA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2o6JkkxsqTf",
        "colab_type": "text"
      },
      "source": [
        "# Import Packages "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pebdmns047PV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run this cell\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "from google.colab import drive \n",
        "#from rdkit import Chem"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBXlZYyscB70",
        "colab_type": "text"
      },
      "source": [
        "# Cloning Repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcehvcAjZb4R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "b57fec58-97ab-4ba0-fc87-2b53bc06596c"
      },
      "source": [
        "!git clone https://github.com/595693085/DGraphDTA.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'DGraphDTA'...\n",
            "remote: Enumerating objects: 372, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/372)\u001b[K\rremote: Counting objects:   1% (4/372)\u001b[K\rremote: Counting objects:   2% (8/372)\u001b[K\rremote: Counting objects:   3% (12/372)\u001b[K\rremote: Counting objects:   4% (15/372)\u001b[K\rremote: Counting objects:   5% (19/372)\u001b[K\rremote: Counting objects:   6% (23/372)\u001b[K\rremote: Counting objects:   7% (27/372)\u001b[K\rremote: Counting objects:   8% (30/372)\u001b[K\rremote: Counting objects:   9% (34/372)\u001b[K\rremote: Counting objects:  10% (38/372)\u001b[K\rremote: Counting objects:  11% (41/372)\u001b[K\rremote: Counting objects:  12% (45/372)\u001b[K\rremote: Counting objects:  13% (49/372)\u001b[K\rremote: Counting objects:  14% (53/372)\u001b[K\rremote: Counting objects:  15% (56/372)\u001b[K\rremote: Counting objects:  16% (60/372)\u001b[K\rremote: Counting objects:  17% (64/372)\u001b[K\rremote: Counting objects:  18% (67/372)\u001b[K\rremote: Counting objects:  19% (71/372)\u001b[K\rremote: Counting objects:  20% (75/372)\u001b[K\rremote: Counting objects:  21% (79/372)\u001b[K\rremote: Counting objects:  22% (82/372)\u001b[K\rremote: Counting objects:  23% (86/372)\u001b[K\rremote: Counting objects:  24% (90/372)\u001b[K\rremote: Counting objects:  25% (93/372)\u001b[K\rremote: Counting objects:  26% (97/372)\u001b[K\rremote: Counting objects:  27% (101/372)\u001b[K\rremote: Counting objects:  28% (105/372)\u001b[K\rremote: Counting objects:  29% (108/372)\u001b[K\rremote: Counting objects:  30% (112/372)\u001b[K\rremote: Counting objects:  31% (116/372)\u001b[K\rremote: Counting objects:  32% (120/372)\u001b[K\rremote: Counting objects:  33% (123/372)\u001b[K\rremote: Counting objects:  34% (127/372)\u001b[K\rremote: Counting objects:  35% (131/372)\u001b[K\rremote: Counting objects:  36% (134/372)\u001b[K\rremote: Counting objects:  37% (138/372)\u001b[K\rremote: Counting objects:  38% (142/372)\u001b[K\rremote: Counting objects:  39% (146/372)\u001b[K\rremote: Counting objects:  40% (149/372)\u001b[K\rremote: Counting objects:  41% (153/372)\u001b[K\rremote: Counting objects:  42% (157/372)\u001b[K\rremote: Counting objects:  43% (160/372)\u001b[K\rremote: Counting objects:  44% (164/372)\u001b[K\rremote: Counting objects:  45% (168/372)\u001b[K\rremote: Counting objects:  46% (172/372)\u001b[K\rremote: Counting objects:  47% (175/372)\u001b[K\rremote: Counting objects:  48% (179/372)\u001b[K\rremote: Counting objects:  49% (183/372)\u001b[K\rremote: Counting objects:  50% (186/372)\u001b[K\rremote: Counting objects:  51% (190/372)\u001b[K\rremote: Counting objects:  52% (194/372)\u001b[K\rremote: Counting objects:  53% (198/372)\u001b[K\rremote: Counting objects:  54% (201/372)\u001b[K\rremote: Counting objects:  55% (205/372)\u001b[K\rremote: Counting objects:  56% (209/372)\u001b[K\rremote: Counting objects:  57% (213/372)\u001b[K\rremote: Counting objects:  58% (216/372)\u001b[K\rremote: Counting objects:  59% (220/372)\u001b[K\rremote: Counting objects:  60% (224/372)\u001b[K\rremote: Counting objects:  61% (227/372)\u001b[K\rremote: Counting objects:  62% (231/372)\u001b[K\rremote: Counting objects:  63% (235/372)\u001b[K\rremote: Counting objects:  64% (239/372)\u001b[K\rremote: Counting objects:  65% (242/372)\u001b[K\rremote: Counting objects:  66% (246/372)\u001b[K\rremote: Counting objects:  67% (250/372)\u001b[K\rremote: Counting objects:  68% (253/372)\u001b[K\rremote: Counting objects:  69% (257/372)\u001b[K\rremote: Counting objects:  70% (261/372)\u001b[K\rremote: Counting objects:  71% (265/372)\u001b[K\rremote: Counting objects:  72% (268/372)\u001b[K\rremote: Counting objects:  73% (272/372)\u001b[K\rremote: Counting objects:  74% (276/372)\u001b[K\rremote: Counting objects:  75% (279/372)\u001b[K\rremote: Counting objects:  76% (283/372)\u001b[K\rremote: Counting objects:  77% (287/372)\u001b[K\rremote: Counting objects:  78% (291/372)\u001b[K\rremote: Counting objects:  79% (294/372)\u001b[K\rremote: Counting objects:  80% (298/372)\u001b[K\rremote: Counting objects:  81% (302/372)\u001b[K\rremote: Counting objects:  82% (306/372)\u001b[K\rremote: Counting objects:  83% (309/372)\u001b[K\rremote: Counting objects:  84% (313/372)\u001b[K\rremote: Counting objects:  85% (317/372)\u001b[K\rremote: Counting objects:  86% (320/372)\u001b[K\rremote: Counting objects:  87% (324/372)\u001b[K\rremote: Counting objects:  88% (328/372)\u001b[K\rremote: Counting objects:  89% (332/372)\u001b[K\rremote: Counting objects:  90% (335/372)\u001b[K\rremote: Counting objects:  91% (339/372)\u001b[K\rremote: Counting objects:  92% (343/372)\u001b[K\rremote: Counting objects:  93% (346/372)\u001b[K\rremote: Counting objects:  94% (350/372)\u001b[K\rremote: Counting objects:  95% (354/372)\u001b[K\rremote: Counting objects:  96% (358/372)\u001b[K\rremote: Counting objects:  97% (361/372)\u001b[K\rremote: Counting objects:  98% (365/372)\u001b[K\rremote: Counting objects:  99% (369/372)\u001b[K\rremote: Counting objects: 100% (372/372)\u001b[K\rremote: Counting objects: 100% (372/372), done.\u001b[K\n",
            "remote: Compressing objects: 100% (341/341), done.\u001b[K\n",
            "remote: Total 372 (delta 105), reused 245 (delta 19), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (372/372), 50.23 MiB | 5.62 MiB/s, done.\n",
            "Resolving deltas: 100% (105/105), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j67nvw6Wb-bR",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessed Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5ym8nVVZjW1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# to avoid the time to process the data\n",
        "\n",
        "!gdown https://drive.google.com/uc?id=1rqAopf_IaH3jzFkwXObQ4i-6bUUwizCv\n",
        "!unzip data.zip\n",
        "!cp -r /content/data/davis/aln /content/DGraphDTA/data/davis\n",
        "!cp -r /content/data/davis/pconsc4 /content/DGraphDTA/data/davis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEUxJ-p52xk6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm data.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYTVP7Q7gWVr",
        "colab_type": "text"
      },
      "source": [
        "# Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtPplcLeYW0j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4d7ac4b2-a915-48cb-8e02-d15b25bba962"
      },
      "source": [
        "!pip3 install cairosvg\n",
        "# !pip3 install numpy Cython pythran && pip3 install pconsc4\n",
        "!pip install torch-geometric torch-scatter==latest+cu101 torch-sparse==latest+cu101 --no-cache-dir -f https://s3.eu-central-1.amazonaws.com/pytorch-geometric.com/whl/torch-1.5.0.html\n",
        "\n",
        "\n",
        "!wget -c https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
        "!time bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
        "!time conda install -q -y -c conda-forge rdkit\n",
        "\n",
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.7/site-packages/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.5)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (0.29.20)\n",
            "Collecting pythran\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/df/d4f805e04d1b26900e284de27c900867ff9acefe67c452aa8ae9bedeb405/pythran-0.9.5.tar.gz (3.6MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6MB 7.2MB/s \n",
            "\u001b[?25hCollecting ply>=3.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/58/35da89ee790598a0700ea49b2a66594140f44dec458c07e8e3d4979137fc/ply-3.11-py2.py3-none-any.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx>=2 in /usr/local/lib/python3.6/dist-packages (from pythran) (2.4)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from pythran) (4.4.2)\n",
            "Requirement already satisfied: gast>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from pythran) (0.3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from pythran) (1.12.0)\n",
            "Collecting beniget>=0.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a7/3f/2b3fe92da80b162c81bd8025aba3751c7c9e7857b7d28f1ccf6e82732162/beniget-0.2.2.tar.gz\n",
            "Building wheels for collected packages: pythran, beniget\n",
            "  Building wheel for pythran (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pythran: filename=pythran-0.9.5-cp36-none-any.whl size=4066521 sha256=0e1496b7eba8fe8090476d7c33fc4a5e712a2d6e5cfd5d31681b267299ab9aca\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/4c/4f/3e8c360280dd141601ddc0cd9f243216f1ef90447faee31764\n",
            "  Building wheel for beniget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for beniget: filename=beniget-0.2.2-cp36-none-any.whl size=9382 sha256=4312ff9d918ca3d00d62232a16259802f588a009eb12d077cb9f0ebfd89ccde7\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/8a/9e/84a2b9ad3b89378e3b6553bf2033bc84afc38dc2d06f98f6f4\n",
            "Successfully built pythran beniget\n",
            "Installing collected packages: ply, beniget, pythran\n",
            "Successfully installed beniget-0.2.2 ply-3.11 pythran-0.9.5\n",
            "Collecting pconsc4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/f1/643b9c03d316982aeaff1baa50a253ac37977a3d359db5321bfabd10334e/pconsc4-0.4.tar.gz (245.6MB)\n",
            "\u001b[K     |████████████████████████████████| 245.6MB 60kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pconsc4) (1.18.5)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from pconsc4) (0.29.20)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pconsc4) (1.4.1)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from pconsc4) (2.3.1)\n",
            "Collecting pyGaussDCA\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/08/8baaba3d31765117b0919389b8f1d92638dc38d424f8b83dc3d48251757f/pyGaussDCA-1.0.tar.gz (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 34.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from pconsc4) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->pconsc4) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->pconsc4) (1.1.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->pconsc4) (1.12.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->pconsc4) (1.0.8)\n",
            "Requirement already satisfied: pythran>=0.8.6 in /usr/local/lib/python3.6/dist-packages (from pyGaussDCA->pconsc4) (0.9.5)\n",
            "Requirement already satisfied: ply>=3.4 in /usr/local/lib/python3.6/dist-packages (from pythran>=0.8.6->pyGaussDCA->pconsc4) (3.11)\n",
            "Requirement already satisfied: beniget>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from pythran>=0.8.6->pyGaussDCA->pconsc4) (0.2.2)\n",
            "Requirement already satisfied: gast>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from pythran>=0.8.6->pyGaussDCA->pconsc4) (0.3.3)\n",
            "Requirement already satisfied: networkx>=2 in /usr/local/lib/python3.6/dist-packages (from pythran>=0.8.6->pyGaussDCA->pconsc4) (2.4)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from pythran>=0.8.6->pyGaussDCA->pconsc4) (4.4.2)\n",
            "Building wheels for collected packages: pconsc4, pyGaussDCA\n",
            "  Building wheel for pconsc4 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pconsc4: filename=pconsc4-0.4-cp36-cp36m-linux_x86_64.whl size=242368212 sha256=861e2a28e0591de45216799c96e7bd072de69f0bad2e2b2fbadec20bd630c66f\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/b7/73/aad30edf27688e0a19c897ddb857528ff9e9533e17bdfcaf2f\n",
            "  Building wheel for pyGaussDCA (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyGaussDCA: filename=pyGaussDCA-1.0-cp36-cp36m-linux_x86_64.whl size=99720 sha256=d02e71aa7c069e89718feac88f127f4f8d120af4a20986333a4dd88bc95d2d22\n",
            "  Stored in directory: /root/.cache/pip/wheels/dc/47/7f/2756916a34cfff276afc01828629721773dcf07ff92504283d\n",
            "Successfully built pconsc4 pyGaussDCA\n",
            "Installing collected packages: pyGaussDCA, pconsc4\n",
            "Successfully installed pconsc4-0.4 pyGaussDCA-1.0\n",
            "Looking in links: https://s3.eu-central-1.amazonaws.com/pytorch-geometric.com/whl/torch-1.5.0.html\n",
            "Collecting torch-geometric\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/f2/26359fb7b50d54924ddd23778d4830b2653df9ffe72f85caad2b829dc778/torch_geometric-1.5.0.tar.gz (153kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 6.6MB/s \n",
            "\u001b[?25hCollecting torch-scatter==latest+cu101\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_scatter-latest%2Bcu101-cp36-cp36m-linux_x86_64.whl (12.2MB)\n",
            "\u001b[K     |████████████████████████████████| 12.3MB 8.5MB/s \n",
            "\u001b[?25hCollecting torch-sparse==latest+cu101\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_sparse-latest%2Bcu101-cp36-cp36m-linux_x86_64.whl (21.6MB)\n",
            "\u001b[K     |████████████████████████████████| 21.6MB 7.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.5.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (4.41.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.22.2.post1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.48.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.23.0)\n",
            "Collecting plyfile\n",
            "  Downloading https://files.pythonhosted.org/packages/93/c8/cf47848cd4d661850e4a8e7f0fc4f7298515e06d0da7255ed08e5312d4aa/plyfile-0.7.2-py3-none-any.whl\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.0.5)\n",
            "Collecting rdflib\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/6b/6454aa1db753c0f8bc265a5bd5c10b5721a4bb24160fb4faf758cf6be8a1/rdflib-5.0.0-py3-none-any.whl (231kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 18.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.10.0)\n",
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.4)\n",
            "Collecting ase\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/70/a8b1a7831193aa228defd805891c534d3e4717c8988147522e673458ddce/ase-3.19.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 23.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torch-geometric) (0.16.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->torch-geometric) (4.4.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->torch-geometric) (0.15.1)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->torch-geometric) (0.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba->torch-geometric) (47.3.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2020.6.20)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2018.9)\n",
            "Collecting isodate\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 42.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (1.12.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (2.4.7)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from ase->torch-geometric) (3.2.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ase->torch-geometric) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ase->torch-geometric) (1.2.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-1.5.0-cp36-none-any.whl size=267918 sha256=3c02aafefb3d3d7eaa521edb054e475d115cbee3c2e890d524e4e3aa0d3ca2b2\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-9dah6yb6/wheels/ec/51/31/5786f2ac419ee312f22d4d2877da05f20e7f2d430e22917daf\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: plyfile, isodate, rdflib, ase, torch-geometric, torch-scatter, torch-sparse\n",
            "Successfully installed ase-3.19.1 isodate-0.6.0 plyfile-0.7.2 rdflib-5.0.0 torch-geometric-1.5.0 torch-scatter-2.0.5 torch-sparse-0.6.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcEOzDr5eRxG",
        "colab_type": "text"
      },
      "source": [
        "# emetrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RS1Etn_eSZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run this cell\n",
        "\n",
        "import numpy as np\n",
        "import subprocess\n",
        "from math import sqrt\n",
        "from sklearn.metrics import average_precision_score\n",
        "from scipy import stats\n",
        " \n",
        " \n",
        "def get_aupr(Y, P, threshold=7.0):\n",
        "    # print(Y.shape,P.shape)\n",
        "    Y = np.where(Y >= 7.0, 1, 0)\n",
        "    P = np.where(P >= 7.0, 1, 0)\n",
        "    aupr = average_precision_score(Y, P)\n",
        "    return aupr\n",
        " \n",
        " \n",
        "def get_cindex(Y, P):\n",
        "    summ = 0\n",
        "    pair = 0\n",
        " \n",
        "    for i in range(1, len(Y)):\n",
        "        for j in range(0, i):\n",
        "            if i is not j:\n",
        "                if (Y[i] > Y[j]):\n",
        "                    pair += 1\n",
        "                    summ += 1 * (P[i] > P[j]) + 0.5 * (P[i] == P[j])\n",
        " \n",
        "    if pair is not 0:\n",
        "        return summ / pair\n",
        "    else:\n",
        "        return 0\n",
        " \n",
        " \n",
        "def r_squared_error(y_obs, y_pred):\n",
        "    y_obs = np.array(y_obs)\n",
        "    y_pred = np.array(y_pred)\n",
        "    y_obs_mean = [np.mean(y_obs) for y in y_obs]\n",
        "    y_pred_mean = [np.mean(y_pred) for y in y_pred]\n",
        " \n",
        "    mult = sum((y_pred - y_pred_mean) * (y_obs - y_obs_mean))\n",
        "    mult = mult * mult\n",
        " \n",
        "    y_obs_sq = sum((y_obs - y_obs_mean) * (y_obs - y_obs_mean))\n",
        "    y_pred_sq = sum((y_pred - y_pred_mean) * (y_pred - y_pred_mean))\n",
        " \n",
        "    return mult / float(y_obs_sq * y_pred_sq)\n",
        " \n",
        " \n",
        "def get_k(y_obs, y_pred):\n",
        "    y_obs = np.array(y_obs)\n",
        "    y_pred = np.array(y_pred)\n",
        " \n",
        "    return sum(y_obs * y_pred) / float(sum(y_pred * y_pred))\n",
        " \n",
        " \n",
        "def squared_error_zero(y_obs, y_pred):\n",
        "    k = get_k(y_obs, y_pred)\n",
        " \n",
        "    y_obs = np.array(y_obs)\n",
        "    y_pred = np.array(y_pred)\n",
        "    y_obs_mean = [np.mean(y_obs) for y in y_obs]\n",
        "    upp = sum((y_obs - (k * y_pred)) * (y_obs - (k * y_pred)))\n",
        "    down = sum((y_obs - y_obs_mean) * (y_obs - y_obs_mean))\n",
        " \n",
        "    return 1 - (upp / float(down))\n",
        " \n",
        " \n",
        "def get_rm2(ys_orig, ys_line):\n",
        "    r2 = r_squared_error(ys_orig, ys_line)\n",
        "    r02 = squared_error_zero(ys_orig, ys_line)\n",
        " \n",
        "    return r2 * (1 - np.sqrt(np.absolute((r2 * r2) - (r02 * r02))))\n",
        " \n",
        " \n",
        "def get_rmse(y, f):\n",
        "    rmse = sqrt(((y - f) ** 2).mean(axis=0))\n",
        "    return rmse\n",
        " \n",
        " \n",
        "def get_mse(y, f):\n",
        "    mse = ((y - f) ** 2).mean(axis=0)\n",
        "    return mse\n",
        " \n",
        " \n",
        "def get_pearson(y, f):\n",
        "    rp = np.corrcoef(y, f)[0, 1]\n",
        "    return rp\n",
        " \n",
        " \n",
        "def get_spearman(y, f):\n",
        "    rs = stats.spearmanr(y, f)[0]\n",
        "    return rs\n",
        " \n",
        " \n",
        "def get_ci(y, f):\n",
        "    ind = np.argsort(y)\n",
        "    y = y[ind]\n",
        "    f = f[ind]\n",
        "    i = len(y) - 1\n",
        "    j = i - 1\n",
        "    z = 0.0\n",
        "    S = 0.0\n",
        "    while i > 0:\n",
        "        while j >= 0:\n",
        "            if y[i] > y[j]:\n",
        "                z = z + 1\n",
        "                u = f[i] - f[j]\n",
        "                if u > 0:\n",
        "                    S = S + 1\n",
        "                elif u == 0:\n",
        "                    S = S + 0.5\n",
        "            j = j - 1\n",
        "        i = i - 1\n",
        "        j = i - 1\n",
        "    ci = S / z\n",
        "    return ci"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17AeReChf384",
        "colab_type": "text"
      },
      "source": [
        "# utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Er96ceCfbK2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run this cell\n",
        "\n",
        "import os\n",
        "from torch_geometric.data import InMemoryDataset, DataLoader, Batch\n",
        "from torch_geometric import data as DATA\n",
        "import torch\n",
        " \n",
        " \n",
        "# initialize the dataset\n",
        "class DTADataset(InMemoryDataset):\n",
        "    def __init__(self, root='/tmp', dataset='davis',\n",
        "                 xd=None, y=None, transform=None,\n",
        "                 pre_transform=None, smile_graph=None, smile_image=None, target_key=None, target_graph=None):\n",
        " \n",
        "        super(DTADataset, self).__init__(root, transform, pre_transform)\n",
        "        self.dataset = dataset\n",
        "        self.process(xd, target_key, y, smile_graph, smile_image, target_graph)\n",
        " \n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        pass\n",
        "        # return ['some_file_1', 'some_file_2', ...]\n",
        " \n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return [self.dataset + '_data_mol_i.pt', self.dataset + '_data_mol_g.pt',self.dataset + '_data_pro.pt']\n",
        " \n",
        "    def download(self):\n",
        "        # Download to `self.raw_dir`.\n",
        "        pass\n",
        " \n",
        "    def _download(self):\n",
        "        pass\n",
        " \n",
        "    def _process(self):\n",
        "        if not os.path.exists(self.processed_dir):\n",
        "            os.makedirs(self.processed_dir)\n",
        " \n",
        "    def process(self, xd, target_key, y, smile_graph, smile_image, target_graph):\n",
        "        assert (len(xd) == len(target_key) and len(xd) == len(y)), 'The three lists must be the same length!'\n",
        "        data_list_mol_i = []\n",
        "        data_list_mol_g = []\n",
        "        data_list_pro = []\n",
        "        data_len = len(xd)\n",
        "        for i in range(data_len):\n",
        "            smiles = xd[i]\n",
        "            tar_key = target_key[i]\n",
        "            labels = y[i]\n",
        "            # convert SMILES to molecular representation using rdkit\n",
        "            c_size, features, edge_index = smile_graph[smiles]\n",
        "            #print('smile in utils', smiles)\n",
        " \n",
        "            drug_image = smile_image[smiles]\n",
        "            flatten = drug_image.reshape(1,300*100)\n",
        " \n",
        "            target_size, target_features, target_edge_index = target_graph[tar_key]\n",
        "            # print(np.array(features).shape, np.array(edge_index).shape)\n",
        "            # print(target_features.shape, target_edge_index.shape)\n",
        "            # make the graph ready for PyTorch Geometrics GCN algorithms:\n",
        "            GCNData_mol_i = DATA.Data(x=torch.Tensor(flatten),\n",
        "                                    y=torch.FloatTensor([labels]))\n",
        "            \n",
        "            GCNData_mol_g = DATA.Data(x=torch.Tensor(features),\n",
        "                                    edge_index=torch.LongTensor(edge_index).transpose(1, 0),\n",
        "                                    y=torch.FloatTensor([labels]))\n",
        "            GCNData_mol_g.__setitem__('c_size', torch.LongTensor([c_size]))\n",
        " \n",
        "            GCNData_pro = DATA.Data(x=torch.Tensor(target_features),\n",
        "                                    edge_index=torch.LongTensor(target_edge_index).transpose(1, 0),\n",
        "                                    y=torch.FloatTensor([labels]))\n",
        "            GCNData_pro.__setitem__('target_size', torch.LongTensor([target_size]))\n",
        "            # print(GCNData.target.size(), GCNData.target_edge_index.size(), GCNData.target_x.size())\n",
        "            data_list_mol_i.append(GCNData_mol_i)\n",
        "            data_list_mol_g.append(GCNData_mol_g)\n",
        "            data_list_pro.append(GCNData_pro)\n",
        " \n",
        "        if self.pre_filter is not None:\n",
        "            data_list_mol_i = [data for data in data_list_mol_i if self.pre_filter(data)]\n",
        "            data_list_mol_g = [data for data in data_list_mol_g if self.pre_filter(data)]\n",
        "            data_list_pro = [data for data in data_list_pro if self.pre_filter(data)]\n",
        "        if self.pre_transform is not None:\n",
        "            data_list_mol_i = [self.pre_transform(data) for data in data_list_mol_i]\n",
        "            data_list_mol_g = [self.pre_transform(data) for data in data_list_mol_g]\n",
        "            data_list_pro = [self.pre_transform(data) for data in data_list_pro]\n",
        "        #self.data_mol_i = data_list_mol_i\n",
        "        self.data_mol_i = data_list_mol_i\n",
        "        self.data_mol_g = data_list_mol_g\n",
        "        self.data_pro = data_list_pro\n",
        " \n",
        "    def __len__(self):\n",
        "        return len(self.data_mol_i)\n",
        " \n",
        "    def __getitem__(self, idx):\n",
        "        #mole_i = load_item(idx, self.data_mol_i)\n",
        "        #mole_g = load_item(idx, self.data_mol_g)\n",
        "        #pro_data = load_item(idx, self.data_pro)\n",
        "        return self.data_mol_i[idx], self.data_mol_g[idx], self.data_pro[idx]\n",
        " \n",
        " \n",
        "# training function at each epoch\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    print('Training on {} samples...'.format(len(train_loader.dataset)))\n",
        "    model.train()\n",
        "    LOG_INTERVAL = 10\n",
        "    TRAIN_BATCH_SIZE = 128\n",
        "    loss_fn = torch.nn.MSELoss()\n",
        "    for batch_idx, data in enumerate(train_loader):\n",
        "        \n",
        "        data_mol_i = data[0].to(device)\n",
        "        data_mol_g = data[1].to(device)\n",
        "        data_pro = data[2].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data_mol_i, data_mol_g, data_pro)\n",
        "        loss = loss_fn(output, data_mol_g.y.view(-1, 1).float().to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % LOG_INTERVAL == 0:\n",
        "            print('Train epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch,\n",
        "                                                                           batch_idx * TRAIN_BATCH_SIZE,\n",
        "                                                                           len(train_loader.dataset),\n",
        "                                                                           100. * batch_idx / len(train_loader),\n",
        "                                                                           loss.item()))\n",
        " \n",
        "# predict\n",
        "def predicting(model, device, loader):\n",
        "    model.eval()\n",
        "    total_preds = torch.Tensor()\n",
        "    total_labels = torch.Tensor()\n",
        "    print('Make prediction for {} samples...'.format(len(loader.dataset)))\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data_mol_i = data[0].to(device)\n",
        "            data_mol_g = data[1].to(device)\n",
        "            data_pro = data[2].to(device)\n",
        "            output = model(data_mol_i, data_mol_g, data_pro)\n",
        "            total_preds = torch.cat((total_preds, output.cpu()), 0)\n",
        "            total_labels = torch.cat((total_labels, data_mol_g.y.view(-1, 1).cpu()), 0)\n",
        "    return total_labels.numpy().flatten(), total_preds.numpy().flatten()\n",
        " \n",
        " \n",
        "#prepare the protein and drug pairs\n",
        "def collate(data_list):\n",
        "    #print('data_list in collate',len(data_list[1]))\n",
        "    batchA = Batch.from_data_list([data[0] for data in data_list])\n",
        "    batchB = Batch.from_data_list([data[1] for data in data_list])\n",
        "    batchC = Batch.from_data_list([data[2] for data in data_list])\n",
        "    return batchA, batchB, batchC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vnj2ZWyueViR"
      },
      "source": [
        "# gnn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "X5rwimj2eViU",
        "colab": {}
      },
      "source": [
        "# run this cell\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, GATConv, global_max_pool as gmp, global_add_pool as gap,global_mean_pool as gep,global_sort_pool\n",
        "from torch_geometric.utils import dropout_adj\n",
        " \n",
        " \n",
        "# GCN based model\n",
        "class GNNNet(torch.nn.Module):\n",
        "    def __init__(self, n_output=1, num_features_pro=54, num_features_mol=78, output_dim=128, dropout=0.2):\n",
        "        super(GNNNet, self).__init__()\n",
        " \n",
        "        print('GNNNet Loaded')\n",
        "        self.n_output = n_output\n",
        "        self.mol_conv1_g = GCNConv(num_features_mol, num_features_mol)\n",
        "        self.mol_conv2_g = GCNConv(num_features_mol, num_features_mol * 2)\n",
        "        self.mol_conv3_g = GCNConv(num_features_mol * 2, num_features_mol * 4)\n",
        "#-------------------------------------------------------------------------------\n",
        "        self.mol_conv1 = nn.Conv2d(3, 32, 2)\n",
        "        self.mol_bn1 = nn.BatchNorm2d(32)\n",
        "        self.mol_conv2 = nn.Conv2d(32, 64, 2)\n",
        "        self.mol_bn2 = nn.BatchNorm2d(64)\n",
        "        self.mol_conv3 = nn.Conv2d(64, 128, 2)\n",
        "        self.mol_bn3 = nn.BatchNorm2d(128)\n",
        "        self.mol_conv4 = nn.Conv2d(128, 256, 2)\n",
        "        self.mol_bn4 = nn.BatchNorm2d(256)\n",
        "        self.mol_conv5 = nn.Conv2d(256, 512, 2)\n",
        "        self.mol_bn5 = nn.BatchNorm2d(512)\n",
        " \n",
        "        self.mol_pool = nn.MaxPool2d(2, 2)\n",
        "        self.drop_rate = dropout\n",
        "        self.mol_fc1 = nn.Linear(512*2*2, 512*2)\n",
        "        self.mol_fc2 = nn.Linear(512*2, 256)\n",
        "        self.mol_fc3 = nn.Linear(256, 128)\n",
        "#-------------------------------------------------------------------------------\n",
        "        self.mol_fc_g1 = torch.nn.Linear(num_features_mol * 4, 1024)\n",
        "        self.mol_fc_g2 = torch.nn.Linear(1024, output_dim)\n",
        " \n",
        "        # self.pro_conv1 = GCNConv(embed_dim, embed_dim)\n",
        "        self.pro_conv1 = GCNConv(num_features_pro, num_features_pro)\n",
        "        self.pro_conv2 = GCNConv(num_features_pro, num_features_pro * 2)\n",
        "        self.pro_conv3 = GCNConv(num_features_pro * 2, num_features_pro * 4)\n",
        "        # self.pro_conv4 = GCNConv(embed_dim * 4, embed_dim * 8)\n",
        "        self.pro_fc_g1 = torch.nn.Linear(num_features_pro * 4, 1024)\n",
        "        self.pro_fc_g2 = torch.nn.Linear(1024, output_dim)\n",
        " \n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        " \n",
        "        # combined layers\n",
        "        #self.fc1 = nn.Linear(1 * output_dim, 1024)    # 2*output_dim\n",
        "        self.fc1 = nn.Linear(128 + output_dim+output_dim, 1024)    # 2*output_dim\n",
        "        self.fc2 = nn.Linear(1024, 512)\n",
        "        self.out = nn.Linear(512, self.n_output)\n",
        " \n",
        "    def forward(self, data_mol, data_mol_g, data_pro):\n",
        "       \n",
        "        # get molecule image input\n",
        "        mol_x_i, mol_edge_index, mol_batch = data_mol.x, data_mol.edge_index, data_mol.batch\n",
        "        size_mol = mol_x_i\n",
        "        size_mol = size_mol.cpu().numpy()\n",
        "        size_mol = size_mol[:,0].size\n",
        "       # mol_x = mol_x.reshape(size_mol[0],3,200,200)\n",
        "        mol_x_i = torch.reshape(mol_x_i, (size_mol,3,100,100))\n",
        "        #mol_x = mol_x.reshape(sample_size,3,200,200)\n",
        " \n",
        "        # get graph input\n",
        "        mol_x, mol_edge_index, mol_batch = data_mol_g.x, data_mol_g.edge_index, data_mol_g.batch\n",
        " \n",
        "        # get protein input\n",
        "        target_x, target_edge_index, target_batch = data_pro.x, data_pro.edge_index, data_pro.batch\n",
        " \n",
        "        # target_seq=data_pro.target\n",
        "#-------------------------------------------------------------------------------\n",
        "        x_g = self.mol_conv1_g(mol_x, mol_edge_index)\n",
        "        x_g = self.relu(x_g)\n",
        " \n",
        "        #mol_edge_index, _ = dropout_adj(mol_edge_index, training=self.training)\n",
        "        x_g = self.mol_conv2_g(x_g, mol_edge_index)\n",
        "        x_g = self.relu(x_g)\n",
        " \n",
        "        # mol_edge_index, _ = dropout_adj(mol_edge_index, training=self.training)\n",
        "        x_g = self.mol_conv3_g(x_g, mol_edge_index)\n",
        "        x_g = self.relu(x_g)\n",
        "        x_g = gep(x_g, mol_batch)  # global pooling\n",
        " \n",
        "        # flatten\n",
        "        x_g = self.relu(self.mol_fc_g1(x_g))\n",
        "        x_g = self.dropout(x_g)\n",
        "        x_g = self.mol_fc_g2(x_g)\n",
        "        x_g = self.dropout(x_g)\n",
        " \n",
        "         # print(x.shape)\n",
        "        x = self.mol_pool(F.relu(self.mol_bn1(self.mol_conv1(mol_x_i))))\n",
        "        # print(x.shape)\n",
        "        x = self.mol_pool(F.relu(self.mol_bn2(self.mol_conv2(x))))\n",
        "        # print(x.shape)\n",
        "        x = self.mol_pool(F.relu(self.mol_bn3(self.mol_conv3(x))))\n",
        "        # print(x.shape)\n",
        "        x = self.mol_pool(F.relu(self.mol_bn4(self.mol_conv4(x))))\n",
        "        # print(x.shape)\n",
        "        x = self.mol_pool(F.relu(self.mol_bn5(self.mol_conv5(x))))\n",
        "        # print(x.shape)\n",
        " \n",
        "       # x = x.view(-1, 32*5*5)\n",
        "        x = x.view(x.size(0), -1)        \n",
        "        x = F.dropout(F.relu(self.mol_fc1(x)), self.drop_rate)\n",
        "        x = F.dropout(F.relu(self.mol_fc2(x)), self.drop_rate)\n",
        "        x = self.mol_fc3(x)\n",
        "#-------------------------------------------------------------------------------\n",
        "        xt = self.pro_conv1(target_x, target_edge_index)\n",
        "        xt = self.relu(xt)\n",
        " \n",
        "        # target_edge_index, _ = dropout_adj(target_edge_index, training=self.training)\n",
        "        xt = self.pro_conv2(xt, target_edge_index)\n",
        "        xt = self.relu(xt)\n",
        " \n",
        "        # target_edge_index, _ = dropout_adj(target_edge_index, training=self.training)\n",
        "        xt = self.pro_conv3(xt, target_edge_index)\n",
        "        xt = self.relu(xt)\n",
        " \n",
        "        # xt = self.pro_conv4(xt, target_edge_index)\n",
        "        # xt = self.relu(xt)\n",
        "        xt = gep(xt, target_batch)  # global pooling\n",
        " \n",
        "        # flatten\n",
        "        xt = self.relu(self.pro_fc_g1(xt))\n",
        "        xt = self.dropout(xt)\n",
        "        xt = self.pro_fc_g2(xt)\n",
        "        xt = self.dropout(xt)\n",
        " \n",
        "        # print(x.size(), xt.size())\n",
        "        # concat\n",
        "        xc = torch.cat((x,x_g, xt), 1)\n",
        "        # add some dense layers\n",
        "        xc = self.fc1(xc)\n",
        "        xc = self.relu(xc)\n",
        "        xc = self.dropout(xc)\n",
        "        xc = self.fc2(xc)\n",
        "        xc = self.relu(xc)\n",
        "        xc = self.dropout(xc)\n",
        "        out = self.out(xc)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fWFBva2tuLI",
        "colab_type": "text"
      },
      "source": [
        "# process with Smile to Image generations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtZLKGlut2cx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run this cell\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import json, pickle\n",
        "from collections import OrderedDict\n",
        "import networkx as nx\n",
        "from rdkit.Chem import MolFromSmiles\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw\n",
        "from rdkit.Chem.Draw import DrawingOptions\n",
        "import cairosvg\n",
        "import sys\n",
        "import cv2\n",
        "import torch\n",
        "import warnings\n",
        "import subprocess\n",
        "from operator import itemgetter\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.sampler import SubsetRandomSampler, BatchSampler, SequentialSampler\n",
        " \n",
        "# from utils import *\n",
        "IMG_SIZE = 100\n",
        "training_files_path = '/root/DGraphDTA/data/davis/SmileImages/'\n",
        " \n",
        "# nomarlize\n",
        "def dic_normalize(dic):\n",
        "    # print(dic)\n",
        "    max_value = dic[max(dic, key=dic.get)]\n",
        "    min_value = dic[min(dic, key=dic.get)]\n",
        "    # print(max_value)\n",
        "    interval = float(max_value) - float(min_value)\n",
        "    for key in dic.keys():\n",
        "        dic[key] = (dic[key] - min_value) / interval\n",
        "    dic['X'] = (max_value + min_value) / 2.0\n",
        "    return dic\n",
        " \n",
        " \n",
        "pro_res_table = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y',\n",
        "                 'X']\n",
        " \n",
        "pro_res_aliphatic_table = ['A', 'I', 'L', 'M', 'V']\n",
        "pro_res_aromatic_table = ['F', 'W', 'Y']\n",
        "pro_res_polar_neutral_table = ['C', 'N', 'Q', 'S', 'T']\n",
        "pro_res_acidic_charged_table = ['D', 'E']\n",
        "pro_res_basic_charged_table = ['H', 'K', 'R']\n",
        " \n",
        "res_weight_table = {'A': 71.08, 'C': 103.15, 'D': 115.09, 'E': 129.12, 'F': 147.18, 'G': 57.05, 'H': 137.14,\n",
        "                    'I': 113.16, 'K': 128.18, 'L': 113.16, 'M': 131.20, 'N': 114.11, 'P': 97.12, 'Q': 128.13,\n",
        "                    'R': 156.19, 'S': 87.08, 'T': 101.11, 'V': 99.13, 'W': 186.22, 'Y': 163.18}\n",
        " \n",
        "res_pka_table = {'A': 2.34, 'C': 1.96, 'D': 1.88, 'E': 2.19, 'F': 1.83, 'G': 2.34, 'H': 1.82, 'I': 2.36,\n",
        "                 'K': 2.18, 'L': 2.36, 'M': 2.28, 'N': 2.02, 'P': 1.99, 'Q': 2.17, 'R': 2.17, 'S': 2.21,\n",
        "                 'T': 2.09, 'V': 2.32, 'W': 2.83, 'Y': 2.32}\n",
        " \n",
        "res_pkb_table = {'A': 9.69, 'C': 10.28, 'D': 9.60, 'E': 9.67, 'F': 9.13, 'G': 9.60, 'H': 9.17,\n",
        "                 'I': 9.60, 'K': 8.95, 'L': 9.60, 'M': 9.21, 'N': 8.80, 'P': 10.60, 'Q': 9.13,\n",
        "                 'R': 9.04, 'S': 9.15, 'T': 9.10, 'V': 9.62, 'W': 9.39, 'Y': 9.62}\n",
        " \n",
        "res_pkx_table = {'A': 0.00, 'C': 8.18, 'D': 3.65, 'E': 4.25, 'F': 0.00, 'G': 0, 'H': 6.00,\n",
        "                 'I': 0.00, 'K': 10.53, 'L': 0.00, 'M': 0.00, 'N': 0.00, 'P': 0.00, 'Q': 0.00,\n",
        "                 'R': 12.48, 'S': 0.00, 'T': 0.00, 'V': 0.00, 'W': 0.00, 'Y': 0.00}\n",
        " \n",
        "res_pl_table = {'A': 6.00, 'C': 5.07, 'D': 2.77, 'E': 3.22, 'F': 5.48, 'G': 5.97, 'H': 7.59,\n",
        "                'I': 6.02, 'K': 9.74, 'L': 5.98, 'M': 5.74, 'N': 5.41, 'P': 6.30, 'Q': 5.65,\n",
        "                'R': 10.76, 'S': 5.68, 'T': 5.60, 'V': 5.96, 'W': 5.89, 'Y': 5.96}\n",
        " \n",
        "res_hydrophobic_ph2_table = {'A': 47, 'C': 52, 'D': -18, 'E': 8, 'F': 92, 'G': 0, 'H': -42, 'I': 100,\n",
        "                             'K': -37, 'L': 100, 'M': 74, 'N': -41, 'P': -46, 'Q': -18, 'R': -26, 'S': -7,\n",
        "                             'T': 13, 'V': 79, 'W': 84, 'Y': 49}\n",
        " \n",
        "res_hydrophobic_ph7_table = {'A': 41, 'C': 49, 'D': -55, 'E': -31, 'F': 100, 'G': 0, 'H': 8, 'I': 99,\n",
        "                             'K': -23, 'L': 97, 'M': 74, 'N': -28, 'P': -46, 'Q': -10, 'R': -14, 'S': -5,\n",
        "                             'T': 13, 'V': 76, 'W': 97, 'Y': 63}\n",
        " \n",
        "res_weight_table = dic_normalize(res_weight_table)\n",
        "res_pka_table = dic_normalize(res_pka_table)\n",
        "res_pkb_table = dic_normalize(res_pkb_table)\n",
        "res_pkx_table = dic_normalize(res_pkx_table)\n",
        "res_pl_table = dic_normalize(res_pl_table)\n",
        "res_hydrophobic_ph2_table = dic_normalize(res_hydrophobic_ph2_table)\n",
        "res_hydrophobic_ph7_table = dic_normalize(res_hydrophobic_ph7_table)\n",
        " \n",
        " \n",
        "# print(res_weight_table)\n",
        " \n",
        " \n",
        "def residue_features(residue):\n",
        "    res_property1 = [1 if residue in pro_res_aliphatic_table else 0, 1 if residue in pro_res_aromatic_table else 0,\n",
        "                     1 if residue in pro_res_polar_neutral_table else 0,\n",
        "                     1 if residue in pro_res_acidic_charged_table else 0,\n",
        "                     1 if residue in pro_res_basic_charged_table else 0]\n",
        "    res_property2 = [res_weight_table[residue], res_pka_table[residue], res_pkb_table[residue], res_pkx_table[residue],\n",
        "                     res_pl_table[residue], res_hydrophobic_ph2_table[residue], res_hydrophobic_ph7_table[residue]]\n",
        "    # print(np.array(res_property1 + res_property2).shape)\n",
        "    return np.array(res_property1 + res_property2)\n",
        " \n",
        " \n",
        "# mol atom feature for mol graph\n",
        "def atom_features(atom):\n",
        "    # 44 +11 +11 +11 +1\n",
        "    return np.array(one_of_k_encoding_unk(atom.GetSymbol(),\n",
        "                                          ['C', 'N', 'O', 'S', 'F', 'Si', 'P', 'Cl', 'Br', 'Mg', 'Na', 'Ca', 'Fe', 'As',\n",
        "                                           'Al', 'I', 'B', 'V', 'K', 'Tl', 'Yb', 'Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se',\n",
        "                                           'Ti', 'Zn', 'H', 'Li', 'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'In', 'Mn', 'Zr', 'Cr',\n",
        "                                           'Pt', 'Hg', 'Pb', 'X']) +\n",
        "                    one_of_k_encoding(atom.GetDegree(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) +\n",
        "                    one_of_k_encoding_unk(atom.GetTotalNumHs(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) +\n",
        "                    one_of_k_encoding_unk(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) +\n",
        "                    [atom.GetIsAromatic()])\n",
        " \n",
        " \n",
        "# one ont encoding\n",
        "def one_of_k_encoding(x, allowable_set):\n",
        "    if x not in allowable_set:\n",
        "        # print(x)\n",
        "        raise Exception('input {0} not in allowable set{1}:'.format(x, allowable_set))\n",
        "    return list(map(lambda s: x == s, allowable_set))\n",
        " \n",
        " \n",
        "def one_of_k_encoding_unk(x, allowable_set):\n",
        "    '''Maps inputs not in the allowable set to the last element.'''\n",
        "    if x not in allowable_set:\n",
        "        x = allowable_set[-1]\n",
        "    return list(map(lambda s: x == s, allowable_set))\n",
        " \n",
        " \n",
        "# mol smile to mol graph edge index\n",
        "def smile_to_graph(smile):\n",
        "    mol = Chem.MolFromSmiles(smile)\n",
        " \n",
        "    c_size = mol.GetNumAtoms()\n",
        " \n",
        "    features = []\n",
        "    for atom in mol.GetAtoms():\n",
        "        feature = atom_features(atom)\n",
        "        features.append(feature / sum(feature))\n",
        " \n",
        "    edges = []\n",
        "    for bond in mol.GetBonds():\n",
        "        edges.append([bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()])\n",
        "    g = nx.Graph(edges).to_directed()\n",
        "    edge_index = []\n",
        "    mol_adj = np.zeros((c_size, c_size))\n",
        "    for e1, e2 in g.edges:\n",
        "        mol_adj[e1, e2] = 1\n",
        "        # edge_index.append([e1, e2])\n",
        "    mol_adj += np.matrix(np.eye(mol_adj.shape[0]))\n",
        "    index_row, index_col = np.where(mol_adj >= 0.5)\n",
        "    for i, j in zip(index_row, index_col):\n",
        "        edge_index.append([i, j])\n",
        "    # print('smile_to_graph')\n",
        "    # print(np.array(features).shape)\n",
        "    return c_size, features, edge_index\n",
        " \n",
        " \n",
        "# smile to images generation\n",
        "def save_comp_imgs_from_smiles(comp_id ,smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    DrawingOptions.atomLabelFontSize = 55\n",
        "    DrawingOptions.dotsPerAngstrom = 100\n",
        "    DrawingOptions.bondLineWidth = 1.5\n",
        "    Draw.MolToFile(mol, os.path.join(training_files_path, \"imgs\", \"{}.png\".format(comp_id)), size= ( IMG_SIZE , IMG_SIZE ))\n",
        " \n",
        "    file_path = os.path.join('/root/DGraphDTA/data/davis/SmileImages/imgs/', comp_id + '.png')\n",
        "  #  print('file path', file_path)\n",
        "    img_arr = cv2.imread(file_path)\n",
        "    angle = random.randint(0,359)\n",
        "    rows, cols, channel = img_arr.shape\n",
        "    rotation_matrix = cv2.getRotationMatrix2D((cols / 2, rows / 2), angle, 1)\n",
        " \n",
        "    img_arr = cv2.warpAffine(img_arr, rotation_matrix, (cols, rows), cv2.INTER_LINEAR,\n",
        "                                                 borderValue=(255, 255, 255))  # cv2.BORDER_CONSTANT, 255)\n",
        " \n",
        "    img_arr = np.array(img_arr) / 255.0\n",
        "    img_arr = img_arr.transpose((2, 0, 1))\n",
        "    new_img = img_arr[0]\n",
        "    new_img = np.concatenate((new_img,img_arr[1]), axis = 0)\n",
        "    new_img = np.concatenate((new_img,img_arr[2]), axis = 0)\n",
        "    return new_img\n",
        " \n",
        "# target feature for target graph\n",
        "def PSSM_calculation(aln_file, pro_seq):\n",
        "    pfm_mat = np.zeros((len(pro_res_table), len(pro_seq)))\n",
        "    with open(aln_file, 'r') as f:\n",
        "        line_count = len(f.readlines())\n",
        "        for line in f.readlines():\n",
        "            if len(line) != len(pro_seq):\n",
        "                print('error', len(line), len(pro_seq))\n",
        "                continue\n",
        "            count = 0\n",
        "            for res in line:\n",
        "                if res not in pro_res_table:\n",
        "                    count += 1\n",
        "                    continue\n",
        "                pfm_mat[pro_res_table.index(res), count] += 1\n",
        "                count += 1\n",
        "    # ppm_mat = pfm_mat / float(line_count)\n",
        "    pseudocount = 0.8\n",
        "    ppm_mat = (pfm_mat + pseudocount / 4) / (float(line_count) + pseudocount)\n",
        "    pssm_mat = ppm_mat\n",
        "    # k = float(len(pro_res_table))\n",
        "    # pwm_mat = np.log2(ppm_mat / (1.0 / k))\n",
        "    # pssm_mat = pwm_mat\n",
        "    # print(pssm_mat)\n",
        "    return pssm_mat\n",
        " \n",
        " \n",
        "def seq_feature(pro_seq):\n",
        "    pro_hot = np.zeros((len(pro_seq), len(pro_res_table)))\n",
        "    pro_property = np.zeros((len(pro_seq), 12))\n",
        "    for i in range(len(pro_seq)):\n",
        "        # if 'X' in pro_seq:\n",
        "        #     print(pro_seq)\n",
        "        pro_hot[i,] = one_of_k_encoding(pro_seq[i], pro_res_table)\n",
        "        pro_property[i,] = residue_features(pro_seq[i])\n",
        "    return np.concatenate((pro_hot, pro_property), axis=1)\n",
        " \n",
        " \n",
        "def target_feature(aln_file, pro_seq):\n",
        "    pssm = PSSM_calculation(aln_file, pro_seq)\n",
        "    other_feature = seq_feature(pro_seq)\n",
        "    # print('target_feature')\n",
        "    # print(pssm.shape)\n",
        "    # print(other_feature.shape)\n",
        " \n",
        "    # print(other_feature.shape)\n",
        "    # return other_feature\n",
        "    return np.concatenate((np.transpose(pssm, (1, 0)), other_feature), axis=1)\n",
        " \n",
        " \n",
        "# target aln file save in data/dataset/aln\n",
        "def target_to_feature(target_key, target_sequence, aln_dir):\n",
        "    # aln_dir = 'data/' + dataset + '/aln'\n",
        "    aln_file = os.path.join(aln_dir, target_key + '.aln')\n",
        "    # if 'X' in target_sequence:\n",
        "    #     print(target_key)\n",
        "    feature = target_feature(aln_file, target_sequence)\n",
        "    return feature\n",
        " \n",
        " \n",
        "# pconsc4 predicted contact map save in data/dataset/pconsc4\n",
        "def target_to_graph(target_key, target_sequence, contact_dir, aln_dir):\n",
        "    target_edge_index = []\n",
        "    target_size = len(target_sequence)\n",
        "    # contact_dir = 'data/' + dataset + '/pconsc4'\n",
        "    contact_file = os.path.join(contact_dir, target_key + '.npy')\n",
        "    contact_map = np.load(contact_file)\n",
        "    contact_map += np.matrix(np.eye(contact_map.shape[0]))\n",
        "    index_row, index_col = np.where(contact_map >= 0.5)\n",
        "    for i, j in zip(index_row, index_col):\n",
        "        target_edge_index.append([i, j])\n",
        "    target_feature = target_to_feature(target_key, target_sequence, aln_dir)\n",
        "    target_edge_index = np.array(target_edge_index)\n",
        "    return target_size, target_feature, target_edge_index\n",
        " \n",
        " \n",
        "# to judge whether the required files exist\n",
        "def valid_target(key, dataset):\n",
        "    contact_dir = '/root/DGraphDTA/data/' + dataset + '/pconsc4'\n",
        "    aln_dir = '/root/DGraphDTA/data/' + dataset + '/aln'\n",
        "    contact_file = os.path.join(contact_dir, key + '.npy')\n",
        "    aln_file = os.path.join(aln_dir, key + '.aln')\n",
        "    # print(contact_file, aln_file)\n",
        "    tc = 0\n",
        "    nc = 0\n",
        "    if os.path.exists(contact_file) and os.path.exists(aln_file):\n",
        "        tc += 1\n",
        "    #    print('true count', contact_file, aln_file)\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        " \n",
        " \n",
        "def data_to_csv(csv_file, datalist):\n",
        "    with open(csv_file, 'w') as f:\n",
        "        f.write('compound_iso_smiles,target_sequence,target_key,affinity\\n')\n",
        "        for data in datalist:\n",
        "            f.write(','.join(map(str, data)) + '\\n')\n",
        " \n",
        " \n",
        "def create_dataset_for_test(dataset):\n",
        "    # load dataset\n",
        "    dataset_path = '/root/DGraphDTA/data/' + dataset + '/'\n",
        "    test_fold = json.load(open(dataset_path + 'folds/test_fold_setting1.txt'))\n",
        "    ligands = json.load(open(dataset_path + 'ligands_can.txt'), object_pairs_hook=OrderedDict)\n",
        "    proteins = json.load(open(dataset_path + 'proteins.txt'), object_pairs_hook=OrderedDict)\n",
        "    affinity = pickle.load(open(dataset_path + 'Y', 'rb'), encoding='latin1')\n",
        "    # load contact and aln\n",
        "    msa_path = '/root/DGraphDTA/data/' + dataset + '/aln'\n",
        "    contac_path = '/root/DGraphDTA/data/' + dataset + '/pconsc4'\n",
        "    msa_list = []\n",
        "    contact_list = []\n",
        "    for key in proteins:\n",
        "        msa_list.append(os.path.join(msa_path, key + '.aln'))\n",
        "        contact_list.append(os.path.join(contac_path, key + '.npy'))\n",
        " \n",
        "    drugs = []\n",
        "    prots = []\n",
        "    prot_keys = []\n",
        "    drug_smiles = []\n",
        "    # smiles\n",
        " \n",
        "    drug_images = {}\n",
        "    for d in ligands.keys():\n",
        "        lg = Chem.MolToSmiles(Chem.MolFromSmiles(ligands[d]), isomericSmiles=True)\n",
        "        drugs.append(lg)\n",
        "        drug_images[lg] = save_comp_imgs_from_smiles(d, lg)\n",
        "        drug_smiles.append(ligands[d])\n",
        " \n",
        "   # drug_images = np.array(drug_images)\n",
        "    \n",
        "    # seqs\n",
        "    for t in proteins.keys():\n",
        "        prots.append(proteins[t])\n",
        "        prot_keys.append(t)\n",
        "    if dataset == 'davis':\n",
        "        affinity = [-np.log10(y / 1e9) for y in affinity]\n",
        "    affinity = np.asarray(affinity)\n",
        " \n",
        "    valid_test_count = 0\n",
        "    rows, cols = np.where(np.isnan(affinity) == False)\n",
        "    rows, cols = rows[test_fold], cols[test_fold]\n",
        "    temp_test_entries = []\n",
        "    print('testfold rows negth',len(rows))                 # should be removed\n",
        "    print('testfold cols negth',len(cols))\n",
        "    for pair_ind in range(len(rows)):\n",
        "        # if the required files is not exist, then pass\n",
        "        if not valid_target(prot_keys[cols[pair_ind]], dataset):\n",
        "            continue\n",
        "        ls = []\n",
        "        ls += [drugs[rows[pair_ind]]]\n",
        "        ls += [prots[cols[pair_ind]]]\n",
        "        ls += [prot_keys[cols[pair_ind]]]\n",
        "        ls += [affinity[rows[pair_ind], cols[pair_ind]]]\n",
        "        temp_test_entries.append(ls)\n",
        "        valid_test_count += 1\n",
        "    csv_file = '/root/DGraphDTA/data/' + dataset + '_test.csv'\n",
        "    data_to_csv(csv_file, temp_test_entries)\n",
        "    print('dataset:', dataset)\n",
        "    print('test entries:', len(test_fold), 'effective test entries', valid_test_count)\n",
        " \n",
        "    compound_iso_smiles = drugs\n",
        "    target_key = prot_keys\n",
        " \n",
        "    # create smile graph\n",
        "    smile_graph = {}\n",
        "    for smile in compound_iso_smiles:\n",
        "        g = smile_to_graph(smile)\n",
        "        smile_graph[smile] = g\n",
        "  #  print(smile_graph['CN1CCN(C(=O)c2cc3cc(Cl)ccc3[nH]2)CC1']) #for test\n",
        " \n",
        "    # create target graph\n",
        "    # print('target_key', len(target_key), len(set(target_key)))\n",
        "    target_graph = {}\n",
        "    \n",
        "    for key in target_key:\n",
        "        if not valid_target(key, dataset):  # ensure the contact and aln files exists\n",
        "            continue\n",
        "        g = target_to_graph(key, proteins[key], contac_path, msa_path)\n",
        "        target_graph[key] = g\n",
        " \n",
        "    # count the number of  proteins with aln and contact files\n",
        "    print('effective drugs,effective prot:', len(smile_graph), len(target_graph))\n",
        "    if len(smile_graph) == 0 or len(target_graph) == 0:\n",
        "        raise Exception('no protein or drug, run the script for datasets preparation.')\n",
        " \n",
        "    # 'data/davis_test.csv' or data/kiba_test.csv'\n",
        "    df_test = pd.read_csv('/root/DGraphDTA/data/' + dataset + '_test.csv')\n",
        "    test_drugs, test_prot_keys, test_Y = list(df_test['compound_iso_smiles']), list(df_test['target_key']), list(\n",
        "        df_test['affinity'])\n",
        "    test_drugs, test_prot_keys, test_Y = np.asarray(test_drugs), np.asarray(test_prot_keys), np.asarray(test_Y)\n",
        "    test_dataset = DTADataset(root='data', dataset=dataset + '_test', xd=test_drugs, y=test_Y,\n",
        "                              target_key=test_prot_keys, smile_graph=smile_graph, smile_image=drug_images, target_graph=target_graph)\n",
        " \n",
        "    return test_dataset\n",
        " \n",
        " \n",
        "def create_dataset_for_5folds(dataset, fold=0):\n",
        "    # load dataset\n",
        "    dataset_path = '/root/DGraphDTA/data/' + dataset + '/'\n",
        "    train_fold_origin = json.load(open(dataset_path + 'folds/train_fold_setting1.txt'))\n",
        "    train_fold_origin = [e for e in train_fold_origin]  # for 5 folds\n",
        " \n",
        "    ligands = json.load(open(dataset_path + 'ligands_can.txt'), object_pairs_hook=OrderedDict)\n",
        "    proteins = json.load(open(dataset_path + 'proteins.txt'), object_pairs_hook=OrderedDict)\n",
        "    # load contact and aln\n",
        "    msa_path = '/root/DGraphDTA/data/' + dataset + '/aln'\n",
        "    contac_path = '/root/DGraphDTA/data/' + dataset + '/pconsc4'\n",
        "    msa_list = []\n",
        "    contact_list = []\n",
        "    for key in proteins:\n",
        "        msa_list.append(os.path.join(msa_path, key + '.aln'))\n",
        "        contact_list.append(os.path.join(contac_path, key + '.npy'))\n",
        "  \n",
        "    # load train,valid and test entries\n",
        "    train_folds = []\n",
        "    valid_fold = train_fold_origin[fold]  # one fold\n",
        "    for i in range(len(train_fold_origin)):  # other folds\n",
        "        if i != fold:\n",
        "            train_folds += train_fold_origin[i]\n",
        "    print('train Folds values', len(train_folds))\n",
        "    affinity = pickle.load(open(dataset_path + 'Y', 'rb'), encoding='latin1')\n",
        "    drugs = []\n",
        "    prots = []\n",
        "    prot_keys = []\n",
        "    drug_smiles = []\n",
        "    drugs_keys = []\n",
        "    # smiles\n",
        "    drug_images = {}\n",
        "    for d in ligands.keys():\n",
        "        lg = Chem.MolToSmiles(Chem.MolFromSmiles(ligands[d]), isomericSmiles=True)\n",
        "        drugs.append(lg)\n",
        "        drug_images[lg] = save_comp_imgs_from_smiles(d, lg)\n",
        "        drug_smiles.append(ligands[d])\n",
        "        drugs_keys.append(d)\n",
        " \n",
        "    # seqs\n",
        "    for t in proteins.keys():\n",
        "        prots.append(proteins[t])\n",
        "        prot_keys.append(t)\n",
        "    \n",
        "    if dataset == 'davis':\n",
        "        affinity = [-np.log10(y / 1e9) for y in affinity]\n",
        "    affinity = np.asarray(affinity)\n",
        " \n",
        "    opts = ['train', 'valid']\n",
        "    valid_train_count = 0\n",
        "    valid_valid_count = 0\n",
        "    for opt in opts:\n",
        "        if opt == 'train':\n",
        "            rows, cols = np.where(np.isnan(affinity) == False)\n",
        "            rows, cols = rows[train_folds], cols[train_folds]\n",
        "            train_fold_entries = []\n",
        "            for pair_ind in range(len(rows)):\n",
        "                if not valid_target(prot_keys[cols[pair_ind]], dataset):  # ensure the contact and aln files exists\n",
        "                    continue\n",
        "                ls = []\n",
        "                \n",
        "                ls += [drugs[rows[pair_ind]]]\n",
        "                ls += [prots[cols[pair_ind]]]\n",
        "                ls += [prot_keys[cols[pair_ind]]]\n",
        "                ls += [affinity[rows[pair_ind], cols[pair_ind]]]\n",
        "           #     ls += [drugs_keys[rows[pair_ind]]]\n",
        "                train_fold_entries.append(ls)\n",
        "                valid_train_count += 1\n",
        " \n",
        "            csv_file = '/root/DGraphDTA/data/' + dataset + '_' + 'fold_' + str(fold) + '_' + opt + '.csv'\n",
        "            data_to_csv(csv_file, train_fold_entries)\n",
        "        elif opt == 'valid':\n",
        "            rows, cols = np.where(np.isnan(affinity) == False)\n",
        "            rows, cols = rows[valid_fold], cols[valid_fold]\n",
        "            valid_fold_entries = []\n",
        "            for pair_ind in range(len(rows)):\n",
        "                if not valid_target(prot_keys[cols[pair_ind]], dataset):\n",
        "                    continue\n",
        "                ls = []\n",
        "                \n",
        "                ls += [drugs[rows[pair_ind]]]\n",
        "                ls += [prots[cols[pair_ind]]]\n",
        "                ls += [prot_keys[cols[pair_ind]]]\n",
        "                ls += [affinity[rows[pair_ind], cols[pair_ind]]]\n",
        "           #     ls += [drugs_keys[rows[pair_ind]]]\n",
        "                valid_fold_entries.append(ls)\n",
        "                valid_valid_count += 1\n",
        " \n",
        "            csv_file = '/root/DGraphDTA/data/' + dataset + '_' + 'fold_' + str(fold) + '_' + opt + '.csv'\n",
        "            data_to_csv(csv_file, valid_fold_entries)\n",
        "    print('dataset:', dataset)\n",
        "    # print('len(set(drugs)),len(set(prots)):', len(set(drugs)), len(set(prots)))\n",
        " \n",
        "    # entries with protein contact and aln files are marked as effiective\n",
        "    print('fold:', fold)\n",
        "    #print('train entries:', len(train_folds), 'effective train entries', valid_train_count)\n",
        "    #print('valid entries:', len(valid_fold), 'effective valid entries', valid_valid_count)\n",
        "    print('train entries:', len(train_folds), 'effective train entries', len(train_fold_entries))\n",
        "    print('valid entries:', len(valid_fold), 'effective valid entries', len(valid_fold_entries))\n",
        " \n",
        "    compound_iso_smiles = drugs\n",
        "    target_key = prot_keys\n",
        "    \n",
        "    # create smile graph\n",
        "    smile_graph = {}\n",
        "    for smile in compound_iso_smiles:\n",
        "        g = smile_to_graph(smile)\n",
        "        smile_graph[smile] = g\n",
        "    # print(smile_graph['CN1CCN(C(=O)c2cc3cc(Cl)ccc3[nH]2)CC1']) #for test\n",
        " \n",
        "    # create target graph\n",
        "    # print('target_key', len(target_key), len(set(target_key)))\n",
        "    target_graph = {}\n",
        "    for key in target_key:\n",
        "        if not valid_target(key, dataset):  # ensure the contact and aln files exists\n",
        "            continue\n",
        "        g = target_to_graph(key, proteins[key], contac_path, msa_path)\n",
        "        target_graph[key] = g\n",
        " \n",
        "    # count the number of  proteins with aln and contact files\n",
        "    print('effective drugs,effective prot:', len(smile_graph), len(target_graph))\n",
        "    if len(smile_graph) == 0 or len(target_graph) == 0:\n",
        "        raise Exception('no protein or drug, run the script for datasets preparation.')\n",
        " \n",
        "    # 'data/davis_fold_0_train.csv' or data/kiba_fold_0__train.csv'\n",
        "    train_csv = '/root/DGraphDTA/data/' + dataset + '_' + 'fold_' + str(fold) + '_' + 'train' + '.csv'\n",
        "    df_train_fold = pd.read_csv(train_csv)\n",
        "    train_drugs, train_prot_keys, train_Y = list(df_train_fold['compound_iso_smiles']), list(\n",
        "        df_train_fold['target_key']), list(df_train_fold['affinity'])\n",
        "    \n",
        " \n",
        "    train_drugs, train_prot_keys, train_Y = np.asarray(train_drugs), np.asarray(train_prot_keys), np.asarray(train_Y)\n",
        "    train_dataset = DTADataset(root='data', dataset=dataset + '_' + 'train', xd=train_drugs, target_key=train_prot_keys,\n",
        "                               y=train_Y, smile_graph=smile_graph, smile_image=drug_images, target_graph=target_graph)\n",
        " \n",
        " \n",
        "    df_valid_fold = pd.read_csv('/root/DGraphDTA/data/' + dataset + '_' + 'fold_' + str(fold) + '_' + 'valid' + '.csv')\n",
        "    valid_drugs, valid_prots_keys, valid_Y = list(df_valid_fold['compound_iso_smiles']), list(\n",
        "        df_valid_fold['target_key']), list(df_valid_fold['affinity'])\n",
        "    valid_drugs, valid_prots_keys, valid_Y = np.asarray(valid_drugs), np.asarray(valid_prots_keys), np.asarray(\n",
        "        valid_Y)\n",
        "    valid_dataset = DTADataset(root='/root/DGraphDTA/data/', dataset=dataset + '_' + 'train', xd=valid_drugs,\n",
        "                               target_key=valid_prots_keys, y=valid_Y, smile_graph=smile_graph, smile_image=drug_images,\n",
        "                               target_graph=target_graph)\n",
        "    return train_dataset, valid_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uts6jhMrH_IO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run this cell\n",
        "\n",
        "!mkdir /root/DGraphDTA/data/davis/SmileImages\n",
        "!mkdir /root/DGraphDTA/data/davis/SmileImages/imgs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_bUiQaOfQBG",
        "colab_type": "text"
      },
      "source": [
        "# training_5folds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsvaPjVEfTUK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1a6d7e23-67ec-42bc-be0c-5ae662e971ff"
      },
      "source": [
        "# run this cell\n",
        "\n",
        "import sys, os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch_geometric.data import DataLoader\n",
        " \n",
        "# from gnn import GNNNet\n",
        "# from utils import *\n",
        "# from emetrics import *\n",
        "# from data_process import create_dataset_for_5folds\n",
        " \n",
        " \n",
        "datasets = ['davis']\n",
        " \n",
        "cuda_name = 'cuda:0'\n",
        "print('cuda_name:', cuda_name)\n",
        "fold = 1 # [0, 1, 2, 3, 4][int(sys.argv[3])]\n",
        "cross_validation_flag = True\n",
        "# print(int(sys.argv[3]))\n",
        " \n",
        "TRAIN_BATCH_SIZE = 384\n",
        "TEST_BATCH_SIZE = 384\n",
        "LR = 0.008\n",
        "NUM_EPOCHS = 2000\n",
        " \n",
        "print('Learning rate: ', LR)\n",
        "print('Epochs: ', NUM_EPOCHS)\n",
        " \n",
        "models_dir = '/content/gdrive/My Drive/DL_Project_local/models'\n",
        "results_dir = '/content/gdrive/My Drive/DL_Project_local/results/'\n",
        " \n",
        "if not os.path.exists(models_dir):\n",
        "    os.makedirs(models_dir)\n",
        " \n",
        "if not os.path.exists(results_dir):\n",
        "    os.makedirs(results_dir)\n",
        " \n",
        "# Main program: iterate over different datasets\n",
        "result_str = ''\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "device = torch.device(cuda_name if USE_CUDA else 'cpu')\n",
        "model = GNNNet()\n",
        "#dummy = torch.load('/content/gdrive/My Drive/DL_Project_local/models/model_GNNNet_CNN_davis_1_0218_282_228epochs_100Image_newModel_p3.model', map_location=cuda_name)\n",
        "#model.load_state_dict(dummy['state_dict'])\n",
        " \n",
        "model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "#optimizer.load_state_dict(dummy['optimizer'])\n",
        "model_st = GNNNet.__name__\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        " \n",
        "loss_mse = []\n",
        "for dataset in datasets:\n",
        "    #train_data, valid_data = create_dataset_for_5folds(dataset, fold)\n",
        "    #train_loader = torch.utils.data.DataLoader(train_data, batch_size=TRAIN_BATCH_SIZE, shuffle=True,\n",
        "    #                                           collate_fn=collate)\n",
        "    #valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=TEST_BATCH_SIZE, shuffle=False,\n",
        "    #                                           collate_fn=collate)\n",
        " \n",
        "    best_mse = 1000\n",
        "    best_test_mse = 1000\n",
        "    best_epoch = -1\n",
        "    #model_file_name = '/content/gdrive/My Drive/DL_Project/' + model_st + '_' + dataset + '_' + str(fold) + '.model'\n",
        "    model_file_name = os.path.join(models_dir,'model_' + model_st + '_' + 'CNN' + '_' + dataset + '_' + str(fold) + '.model')\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "\n",
        "        train_data, valid_data = create_dataset_for_5folds(dataset, fold)\n",
        "        train_loader = torch.utils.data.DataLoader(train_data, batch_size=TRAIN_BATCH_SIZE, shuffle=True,\n",
        "                                               collate_fn=collate)\n",
        "        valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=TEST_BATCH_SIZE, shuffle=False,\n",
        "                                               collate_fn=collate)\n",
        "    \n",
        "        train(model, device, train_loader, optimizer, epoch + 1)\n",
        "        print('predicting for valid data')\n",
        "        G, P = predicting(model, device, valid_loader)\n",
        "        val = get_mse(G, P)\n",
        "        loss_mse.append(val)\n",
        "        print('valid result:', val, best_mse)\n",
        "        if val < best_mse:\n",
        "            best_mse = val\n",
        "            best_epoch = epoch + 1\n",
        "             #########################\n",
        "            state = {\n",
        "                    'epoch': epoch,\n",
        "                    'state_dict': model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                }\n",
        "            torch.save(state, model_file_name)\n",
        "            #########################\n",
        "            np.save(results_dir+'CNN_GNN',np.array(loss_mse))\n",
        "            \n",
        "            #torch.save(model.state_dict(), model_file_name)\n",
        "            print('rmse improved at epoch ', best_epoch, '; best_test_mse', best_mse, model_st, dataset, fold)\n",
        "        else:\n",
        "            print('No improvement since epoch ', best_epoch, '; best_test_mse', best_mse, model_st, dataset, fold)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda_name: cuda:0\n",
            "Learning rate:  0.008\n",
            "Epochs:  2000\n",
            "GNNNet Loaded\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 1 [0/10000 (0%)]\tLoss: 30.662472\n",
            "Train epoch: 1 [1280/10000 (37%)]\tLoss: 8.253242\n",
            "Train epoch: 1 [2560/10000 (74%)]\tLoss: 4.057731\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 6.108784 1000\n",
            "rmse improved at epoch  1 ; best_test_mse 6.108784 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 2 [0/10000 (0%)]\tLoss: 3.221485\n",
            "Train epoch: 2 [1280/10000 (37%)]\tLoss: 2.497167\n",
            "Train epoch: 2 [2560/10000 (74%)]\tLoss: 2.181303\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 2.4866977 6.108784\n",
            "rmse improved at epoch  2 ; best_test_mse 2.4866977 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 3 [0/10000 (0%)]\tLoss: 2.155302\n",
            "Train epoch: 3 [1280/10000 (37%)]\tLoss: 1.502276\n",
            "Train epoch: 3 [2560/10000 (74%)]\tLoss: 1.426332\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.1823763 2.4866977\n",
            "rmse improved at epoch  3 ; best_test_mse 1.1823763 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 4 [0/10000 (0%)]\tLoss: 1.841471\n",
            "Train epoch: 4 [1280/10000 (37%)]\tLoss: 1.286005\n",
            "Train epoch: 4 [2560/10000 (74%)]\tLoss: 1.022707\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.90837 1.1823763\n",
            "rmse improved at epoch  4 ; best_test_mse 0.90837 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 5 [0/10000 (0%)]\tLoss: 1.158876\n",
            "Train epoch: 5 [1280/10000 (37%)]\tLoss: 1.330571\n",
            "Train epoch: 5 [2560/10000 (74%)]\tLoss: 1.178026\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.6083647 0.90837\n",
            "No improvement since epoch  4 ; best_test_mse 0.90837 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 6 [0/10000 (0%)]\tLoss: 1.731980\n",
            "Train epoch: 6 [1280/10000 (37%)]\tLoss: 1.519732\n",
            "Train epoch: 6 [2560/10000 (74%)]\tLoss: 0.880725\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.9849501 0.90837\n",
            "No improvement since epoch  4 ; best_test_mse 0.90837 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 7 [0/10000 (0%)]\tLoss: 1.140061\n",
            "Train epoch: 7 [1280/10000 (37%)]\tLoss: 1.104796\n",
            "Train epoch: 7 [2560/10000 (74%)]\tLoss: 1.331127\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.3130244 0.90837\n",
            "No improvement since epoch  4 ; best_test_mse 0.90837 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 8 [0/10000 (0%)]\tLoss: 1.112736\n",
            "Train epoch: 8 [1280/10000 (37%)]\tLoss: 1.153420\n",
            "Train epoch: 8 [2560/10000 (74%)]\tLoss: 1.251565\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 2.8860765 0.90837\n",
            "No improvement since epoch  4 ; best_test_mse 0.90837 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 9 [0/10000 (0%)]\tLoss: 2.108061\n",
            "Train epoch: 9 [1280/10000 (37%)]\tLoss: 1.123076\n",
            "Train epoch: 9 [2560/10000 (74%)]\tLoss: 1.387927\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.4096724 0.90837\n",
            "No improvement since epoch  4 ; best_test_mse 0.90837 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 10 [0/10000 (0%)]\tLoss: 1.337808\n",
            "Train epoch: 10 [1280/10000 (37%)]\tLoss: 1.129489\n",
            "Train epoch: 10 [2560/10000 (74%)]\tLoss: 1.101395\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.9832812 0.90837\n",
            "No improvement since epoch  4 ; best_test_mse 0.90837 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 11 [0/10000 (0%)]\tLoss: 1.126371\n",
            "Train epoch: 11 [1280/10000 (37%)]\tLoss: 0.986339\n",
            "Train epoch: 11 [2560/10000 (74%)]\tLoss: 1.032519\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.5729284 0.90837\n",
            "No improvement since epoch  4 ; best_test_mse 0.90837 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 12 [0/10000 (0%)]\tLoss: 1.050845\n",
            "Train epoch: 12 [1280/10000 (37%)]\tLoss: 1.047991\n",
            "Train epoch: 12 [2560/10000 (74%)]\tLoss: 1.140549\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.87116325 0.90837\n",
            "rmse improved at epoch  12 ; best_test_mse 0.87116325 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 13 [0/10000 (0%)]\tLoss: 2.405738\n",
            "Train epoch: 13 [1280/10000 (37%)]\tLoss: 2.000291\n",
            "Train epoch: 13 [2560/10000 (74%)]\tLoss: 1.184936\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.8302929 0.87116325\n",
            "No improvement since epoch  12 ; best_test_mse 0.87116325 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 14 [0/10000 (0%)]\tLoss: 1.677559\n",
            "Train epoch: 14 [1280/10000 (37%)]\tLoss: 0.958662\n",
            "Train epoch: 14 [2560/10000 (74%)]\tLoss: 0.945214\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.5149559 0.87116325\n",
            "No improvement since epoch  12 ; best_test_mse 0.87116325 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 15 [0/10000 (0%)]\tLoss: 1.372132\n",
            "Train epoch: 15 [1280/10000 (37%)]\tLoss: 0.915143\n",
            "Train epoch: 15 [2560/10000 (74%)]\tLoss: 1.194286\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.0846965 0.87116325\n",
            "No improvement since epoch  12 ; best_test_mse 0.87116325 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 16 [0/10000 (0%)]\tLoss: 0.985523\n",
            "Train epoch: 16 [1280/10000 (37%)]\tLoss: 1.260657\n",
            "Train epoch: 16 [2560/10000 (74%)]\tLoss: 0.951469\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.3820785 0.87116325\n",
            "No improvement since epoch  12 ; best_test_mse 0.87116325 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 17 [0/10000 (0%)]\tLoss: 1.111844\n",
            "Train epoch: 17 [1280/10000 (37%)]\tLoss: 1.023097\n",
            "Train epoch: 17 [2560/10000 (74%)]\tLoss: 1.152887\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.7568171 0.87116325\n",
            "rmse improved at epoch  17 ; best_test_mse 0.7568171 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 18 [0/10000 (0%)]\tLoss: 1.417632\n",
            "Train epoch: 18 [1280/10000 (37%)]\tLoss: 1.198018\n",
            "Train epoch: 18 [2560/10000 (74%)]\tLoss: 0.960760\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.819865 0.7568171\n",
            "No improvement since epoch  17 ; best_test_mse 0.7568171 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 19 [0/10000 (0%)]\tLoss: 1.955647\n",
            "Train epoch: 19 [1280/10000 (37%)]\tLoss: 1.284722\n",
            "Train epoch: 19 [2560/10000 (74%)]\tLoss: 1.006321\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.3905824 0.7568171\n",
            "No improvement since epoch  17 ; best_test_mse 0.7568171 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 20 [0/10000 (0%)]\tLoss: 0.957145\n",
            "Train epoch: 20 [1280/10000 (37%)]\tLoss: 0.991142\n",
            "Train epoch: 20 [2560/10000 (74%)]\tLoss: 1.071708\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.6121209 0.7568171\n",
            "No improvement since epoch  17 ; best_test_mse 0.7568171 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 21 [0/10000 (0%)]\tLoss: 1.203138\n",
            "Train epoch: 21 [1280/10000 (37%)]\tLoss: 1.130172\n",
            "Train epoch: 21 [2560/10000 (74%)]\tLoss: 0.964634\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.7899177 0.7568171\n",
            "No improvement since epoch  17 ; best_test_mse 0.7568171 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 22 [0/10000 (0%)]\tLoss: 2.052005\n",
            "Train epoch: 22 [1280/10000 (37%)]\tLoss: 1.126587\n",
            "Train epoch: 22 [2560/10000 (74%)]\tLoss: 1.139026\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.0681586 0.7568171\n",
            "No improvement since epoch  17 ; best_test_mse 0.7568171 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 23 [0/10000 (0%)]\tLoss: 0.999337\n",
            "Train epoch: 23 [1280/10000 (37%)]\tLoss: 0.821676\n",
            "Train epoch: 23 [2560/10000 (74%)]\tLoss: 0.957525\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.8615328 0.7568171\n",
            "No improvement since epoch  17 ; best_test_mse 0.7568171 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 24 [0/10000 (0%)]\tLoss: 1.052621\n",
            "Train epoch: 24 [1280/10000 (37%)]\tLoss: 1.206444\n",
            "Train epoch: 24 [2560/10000 (74%)]\tLoss: 1.147479\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.6357938 0.7568171\n",
            "No improvement since epoch  17 ; best_test_mse 0.7568171 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 25 [0/10000 (0%)]\tLoss: 1.089406\n",
            "Train epoch: 25 [1280/10000 (37%)]\tLoss: 0.961270\n",
            "Train epoch: 25 [2560/10000 (74%)]\tLoss: 1.049291\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.1154313 0.7568171\n",
            "No improvement since epoch  17 ; best_test_mse 0.7568171 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 26 [0/10000 (0%)]\tLoss: 0.846382\n",
            "Train epoch: 26 [1280/10000 (37%)]\tLoss: 0.861704\n",
            "Train epoch: 26 [2560/10000 (74%)]\tLoss: 0.959780\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.7143949 0.7568171\n",
            "rmse improved at epoch  26 ; best_test_mse 0.7143949 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 27 [0/10000 (0%)]\tLoss: 1.448115\n",
            "Train epoch: 27 [1280/10000 (37%)]\tLoss: 1.106078\n",
            "Train epoch: 27 [2560/10000 (74%)]\tLoss: 0.817905\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.81208175 0.7143949\n",
            "No improvement since epoch  26 ; best_test_mse 0.7143949 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 28 [0/10000 (0%)]\tLoss: 1.002564\n",
            "Train epoch: 28 [1280/10000 (37%)]\tLoss: 0.885667\n",
            "Train epoch: 28 [2560/10000 (74%)]\tLoss: 0.936484\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.8745192 0.7143949\n",
            "No improvement since epoch  26 ; best_test_mse 0.7143949 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 29 [0/10000 (0%)]\tLoss: 1.151321\n",
            "Train epoch: 29 [1280/10000 (37%)]\tLoss: 0.999216\n",
            "Train epoch: 29 [2560/10000 (74%)]\tLoss: 1.120903\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.3841006 0.7143949\n",
            "No improvement since epoch  26 ; best_test_mse 0.7143949 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 30 [0/10000 (0%)]\tLoss: 0.819691\n",
            "Train epoch: 30 [1280/10000 (37%)]\tLoss: 0.943744\n",
            "Train epoch: 30 [2560/10000 (74%)]\tLoss: 0.797825\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.3651855 0.7143949\n",
            "No improvement since epoch  26 ; best_test_mse 0.7143949 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 31 [0/10000 (0%)]\tLoss: 0.839969\n",
            "Train epoch: 31 [1280/10000 (37%)]\tLoss: 1.192156\n",
            "Train epoch: 31 [2560/10000 (74%)]\tLoss: 0.875515\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.7537158 0.7143949\n",
            "No improvement since epoch  26 ; best_test_mse 0.7143949 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 32 [0/10000 (0%)]\tLoss: 1.178364\n",
            "Train epoch: 32 [1280/10000 (37%)]\tLoss: 0.706948\n",
            "Train epoch: 32 [2560/10000 (74%)]\tLoss: 0.910737\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.71456736 0.7143949\n",
            "No improvement since epoch  26 ; best_test_mse 0.7143949 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 33 [0/10000 (0%)]\tLoss: 1.767972\n",
            "Train epoch: 33 [1280/10000 (37%)]\tLoss: 0.884721\n",
            "Train epoch: 33 [2560/10000 (74%)]\tLoss: 1.007373\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.79418844 0.7143949\n",
            "No improvement since epoch  26 ; best_test_mse 0.7143949 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 34 [0/10000 (0%)]\tLoss: 1.008278\n",
            "Train epoch: 34 [1280/10000 (37%)]\tLoss: 0.890610\n",
            "Train epoch: 34 [2560/10000 (74%)]\tLoss: 0.829930\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.0236988 0.7143949\n",
            "No improvement since epoch  26 ; best_test_mse 0.7143949 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 35 [0/10000 (0%)]\tLoss: 0.682764\n",
            "Train epoch: 35 [1280/10000 (37%)]\tLoss: 0.930283\n",
            "Train epoch: 35 [2560/10000 (74%)]\tLoss: 0.862594\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.2720307 0.7143949\n",
            "No improvement since epoch  26 ; best_test_mse 0.7143949 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 36 [0/10000 (0%)]\tLoss: 0.762343\n",
            "Train epoch: 36 [1280/10000 (37%)]\tLoss: 0.975568\n",
            "Train epoch: 36 [2560/10000 (74%)]\tLoss: 0.834947\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.7952299 0.7143949\n",
            "No improvement since epoch  26 ; best_test_mse 0.7143949 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 37 [0/10000 (0%)]\tLoss: 0.977227\n",
            "Train epoch: 37 [1280/10000 (37%)]\tLoss: 0.864839\n",
            "Train epoch: 37 [2560/10000 (74%)]\tLoss: 1.044549\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 2.113866 0.7143949\n",
            "No improvement since epoch  26 ; best_test_mse 0.7143949 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 38 [0/10000 (0%)]\tLoss: 1.261040\n",
            "Train epoch: 38 [1280/10000 (37%)]\tLoss: 0.937217\n",
            "Train epoch: 38 [2560/10000 (74%)]\tLoss: 1.048166\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.4514489 0.7143949\n",
            "No improvement since epoch  26 ; best_test_mse 0.7143949 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 39 [0/10000 (0%)]\tLoss: 0.896944\n",
            "Train epoch: 39 [1280/10000 (37%)]\tLoss: 0.854277\n",
            "Train epoch: 39 [2560/10000 (74%)]\tLoss: 0.929306\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.3728781 0.7143949\n",
            "No improvement since epoch  26 ; best_test_mse 0.7143949 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 40 [0/10000 (0%)]\tLoss: 0.955096\n",
            "Train epoch: 40 [1280/10000 (37%)]\tLoss: 0.982501\n",
            "Train epoch: 40 [2560/10000 (74%)]\tLoss: 0.890483\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.2685703 0.7143949\n",
            "No improvement since epoch  26 ; best_test_mse 0.7143949 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 41 [0/10000 (0%)]\tLoss: 0.749004\n",
            "Train epoch: 41 [1280/10000 (37%)]\tLoss: 0.868642\n",
            "Train epoch: 41 [2560/10000 (74%)]\tLoss: 1.005777\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.3116497 0.7143949\n",
            "No improvement since epoch  26 ; best_test_mse 0.7143949 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 42 [0/10000 (0%)]\tLoss: 0.849098\n",
            "Train epoch: 42 [1280/10000 (37%)]\tLoss: 1.067786\n",
            "Train epoch: 42 [2560/10000 (74%)]\tLoss: 1.021121\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.68881345 0.7143949\n",
            "rmse improved at epoch  42 ; best_test_mse 0.68881345 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 43 [0/10000 (0%)]\tLoss: 1.302430\n",
            "Train epoch: 43 [1280/10000 (37%)]\tLoss: 1.011986\n",
            "Train epoch: 43 [2560/10000 (74%)]\tLoss: 1.111159\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.5063037 0.68881345\n",
            "No improvement since epoch  42 ; best_test_mse 0.68881345 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 44 [0/10000 (0%)]\tLoss: 0.947447\n",
            "Train epoch: 44 [1280/10000 (37%)]\tLoss: 0.905928\n",
            "Train epoch: 44 [2560/10000 (74%)]\tLoss: 0.724262\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.3873081 0.68881345\n",
            "No improvement since epoch  42 ; best_test_mse 0.68881345 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 45 [0/10000 (0%)]\tLoss: 0.849140\n",
            "Train epoch: 45 [1280/10000 (37%)]\tLoss: 0.793310\n",
            "Train epoch: 45 [2560/10000 (74%)]\tLoss: 0.734019\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.65009105 0.68881345\n",
            "rmse improved at epoch  45 ; best_test_mse 0.65009105 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 46 [0/10000 (0%)]\tLoss: 1.047485\n",
            "Train epoch: 46 [1280/10000 (37%)]\tLoss: 0.951624\n",
            "Train epoch: 46 [2560/10000 (74%)]\tLoss: 0.791716\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.9416465 0.65009105\n",
            "No improvement since epoch  45 ; best_test_mse 0.65009105 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 47 [0/10000 (0%)]\tLoss: 1.347231\n",
            "Train epoch: 47 [1280/10000 (37%)]\tLoss: 0.797123\n",
            "Train epoch: 47 [2560/10000 (74%)]\tLoss: 0.800411\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.6522316 0.65009105\n",
            "No improvement since epoch  45 ; best_test_mse 0.65009105 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 48 [0/10000 (0%)]\tLoss: 1.666810\n",
            "Train epoch: 48 [1280/10000 (37%)]\tLoss: 1.164011\n",
            "Train epoch: 48 [2560/10000 (74%)]\tLoss: 0.864996\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.0578357 0.65009105\n",
            "No improvement since epoch  45 ; best_test_mse 0.65009105 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 49 [0/10000 (0%)]\tLoss: 0.817664\n",
            "Train epoch: 49 [1280/10000 (37%)]\tLoss: 0.796936\n",
            "Train epoch: 49 [2560/10000 (74%)]\tLoss: 0.931034\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.4671485 0.65009105\n",
            "No improvement since epoch  45 ; best_test_mse 0.65009105 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 50 [0/10000 (0%)]\tLoss: 0.924539\n",
            "Train epoch: 50 [1280/10000 (37%)]\tLoss: 1.022089\n",
            "Train epoch: 50 [2560/10000 (74%)]\tLoss: 0.828257\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.6104313 0.65009105\n",
            "No improvement since epoch  45 ; best_test_mse 0.65009105 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 51 [0/10000 (0%)]\tLoss: 0.924806\n",
            "Train epoch: 51 [1280/10000 (37%)]\tLoss: 0.774914\n",
            "Train epoch: 51 [2560/10000 (74%)]\tLoss: 1.023263\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.2166858 0.65009105\n",
            "No improvement since epoch  45 ; best_test_mse 0.65009105 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 52 [0/10000 (0%)]\tLoss: 0.719088\n",
            "Train epoch: 52 [1280/10000 (37%)]\tLoss: 1.150440\n",
            "Train epoch: 52 [2560/10000 (74%)]\tLoss: 0.928252\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.6520708 0.65009105\n",
            "No improvement since epoch  45 ; best_test_mse 0.65009105 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 53 [0/10000 (0%)]\tLoss: 1.081326\n",
            "Train epoch: 53 [1280/10000 (37%)]\tLoss: 0.889706\n",
            "Train epoch: 53 [2560/10000 (74%)]\tLoss: 0.750925\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.98910147 0.65009105\n",
            "No improvement since epoch  45 ; best_test_mse 0.65009105 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 54 [0/10000 (0%)]\tLoss: 0.766012\n",
            "Train epoch: 54 [1280/10000 (37%)]\tLoss: 0.859217\n",
            "Train epoch: 54 [2560/10000 (74%)]\tLoss: 0.750617\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.4079173 0.65009105\n",
            "No improvement since epoch  45 ; best_test_mse 0.65009105 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 55 [0/10000 (0%)]\tLoss: 0.935627\n",
            "Train epoch: 55 [1280/10000 (37%)]\tLoss: 0.845703\n",
            "Train epoch: 55 [2560/10000 (74%)]\tLoss: 0.646380\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.6393715 0.65009105\n",
            "rmse improved at epoch  55 ; best_test_mse 0.6393715 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 56 [0/10000 (0%)]\tLoss: 1.333438\n",
            "Train epoch: 56 [1280/10000 (37%)]\tLoss: 0.942518\n",
            "Train epoch: 56 [2560/10000 (74%)]\tLoss: 0.758791\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.71002287 0.6393715\n",
            "No improvement since epoch  55 ; best_test_mse 0.6393715 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 57 [0/10000 (0%)]\tLoss: 0.963866\n",
            "Train epoch: 57 [1280/10000 (37%)]\tLoss: 0.705123\n",
            "Train epoch: 57 [2560/10000 (74%)]\tLoss: 0.757012\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.80984944 0.6393715\n",
            "No improvement since epoch  55 ; best_test_mse 0.6393715 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 58 [0/10000 (0%)]\tLoss: 0.885532\n",
            "Train epoch: 58 [1280/10000 (37%)]\tLoss: 0.767037\n",
            "Train epoch: 58 [2560/10000 (74%)]\tLoss: 0.761448\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.6903114 0.6393715\n",
            "No improvement since epoch  55 ; best_test_mse 0.6393715 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 59 [0/10000 (0%)]\tLoss: 1.691968\n",
            "Train epoch: 59 [1280/10000 (37%)]\tLoss: 0.907206\n",
            "Train epoch: 59 [2560/10000 (74%)]\tLoss: 0.660321\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.1533874 0.6393715\n",
            "No improvement since epoch  55 ; best_test_mse 0.6393715 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 60 [0/10000 (0%)]\tLoss: 0.792794\n",
            "Train epoch: 60 [1280/10000 (37%)]\tLoss: 0.690993\n",
            "Train epoch: 60 [2560/10000 (74%)]\tLoss: 0.608070\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.0262715 0.6393715\n",
            "No improvement since epoch  55 ; best_test_mse 0.6393715 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 61 [0/10000 (0%)]\tLoss: 0.843774\n",
            "Train epoch: 61 [1280/10000 (37%)]\tLoss: 0.787098\n",
            "Train epoch: 61 [2560/10000 (74%)]\tLoss: 0.853786\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.0683379 0.6393715\n",
            "No improvement since epoch  55 ; best_test_mse 0.6393715 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 62 [0/10000 (0%)]\tLoss: 0.813345\n",
            "Train epoch: 62 [1280/10000 (37%)]\tLoss: 0.862771\n",
            "Train epoch: 62 [2560/10000 (74%)]\tLoss: 0.886065\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.2844378 0.6393715\n",
            "No improvement since epoch  55 ; best_test_mse 0.6393715 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 63 [0/10000 (0%)]\tLoss: 0.712537\n",
            "Train epoch: 63 [1280/10000 (37%)]\tLoss: 0.730331\n",
            "Train epoch: 63 [2560/10000 (74%)]\tLoss: 0.746886\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.95429534 0.6393715\n",
            "No improvement since epoch  55 ; best_test_mse 0.6393715 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 64 [0/10000 (0%)]\tLoss: 0.911064\n",
            "Train epoch: 64 [1280/10000 (37%)]\tLoss: 0.690330\n",
            "Train epoch: 64 [2560/10000 (74%)]\tLoss: 0.797529\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.6731831 0.6393715\n",
            "No improvement since epoch  55 ; best_test_mse 0.6393715 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 65 [0/10000 (0%)]\tLoss: 0.960520\n",
            "Train epoch: 65 [1280/10000 (37%)]\tLoss: 0.779171\n",
            "Train epoch: 65 [2560/10000 (74%)]\tLoss: 0.757538\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.0700607 0.6393715\n",
            "No improvement since epoch  55 ; best_test_mse 0.6393715 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 66 [0/10000 (0%)]\tLoss: 0.737627\n",
            "Train epoch: 66 [1280/10000 (37%)]\tLoss: 0.912493\n",
            "Train epoch: 66 [2560/10000 (74%)]\tLoss: 0.938119\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.8659856 0.6393715\n",
            "No improvement since epoch  55 ; best_test_mse 0.6393715 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 67 [0/10000 (0%)]\tLoss: 0.829274\n",
            "Train epoch: 67 [1280/10000 (37%)]\tLoss: 0.641125\n",
            "Train epoch: 67 [2560/10000 (74%)]\tLoss: 0.931202\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.5043695 0.6393715\n",
            "No improvement since epoch  55 ; best_test_mse 0.6393715 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 68 [0/10000 (0%)]\tLoss: 0.820672\n",
            "Train epoch: 68 [1280/10000 (37%)]\tLoss: 0.832955\n",
            "Train epoch: 68 [2560/10000 (74%)]\tLoss: 0.861504\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.7898275 0.6393715\n",
            "No improvement since epoch  55 ; best_test_mse 0.6393715 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 69 [0/10000 (0%)]\tLoss: 0.654421\n",
            "Train epoch: 69 [1280/10000 (37%)]\tLoss: 0.865638\n",
            "Train epoch: 69 [2560/10000 (74%)]\tLoss: 0.676745\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.4517106 0.6393715\n",
            "No improvement since epoch  55 ; best_test_mse 0.6393715 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 70 [0/10000 (0%)]\tLoss: 0.944910\n",
            "Train epoch: 70 [1280/10000 (37%)]\tLoss: 0.825455\n",
            "Train epoch: 70 [2560/10000 (74%)]\tLoss: 0.664042\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.67769146 0.6393715\n",
            "No improvement since epoch  55 ; best_test_mse 0.6393715 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 71 [0/10000 (0%)]\tLoss: 0.853138\n",
            "Train epoch: 71 [1280/10000 (37%)]\tLoss: 0.625978\n",
            "Train epoch: 71 [2560/10000 (74%)]\tLoss: 0.803341\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.3144034 0.6393715\n",
            "No improvement since epoch  55 ; best_test_mse 0.6393715 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 72 [0/10000 (0%)]\tLoss: 0.877313\n",
            "Train epoch: 72 [1280/10000 (37%)]\tLoss: 0.728170\n",
            "Train epoch: 72 [2560/10000 (74%)]\tLoss: 0.796941\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.2683245 0.6393715\n",
            "No improvement since epoch  55 ; best_test_mse 0.6393715 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 73 [0/10000 (0%)]\tLoss: 0.725511\n",
            "Train epoch: 73 [1280/10000 (37%)]\tLoss: 0.816503\n",
            "Train epoch: 73 [2560/10000 (74%)]\tLoss: 0.652110\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.87243706 0.6393715\n",
            "No improvement since epoch  55 ; best_test_mse 0.6393715 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 74 [0/10000 (0%)]\tLoss: 0.704358\n",
            "Train epoch: 74 [1280/10000 (37%)]\tLoss: 0.841375\n",
            "Train epoch: 74 [2560/10000 (74%)]\tLoss: 0.770867\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.9948154 0.6393715\n",
            "No improvement since epoch  55 ; best_test_mse 0.6393715 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 75 [0/10000 (0%)]\tLoss: 0.734984\n",
            "Train epoch: 75 [1280/10000 (37%)]\tLoss: 0.791760\n",
            "Train epoch: 75 [2560/10000 (74%)]\tLoss: 0.807295\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.1773487 0.6393715\n",
            "No improvement since epoch  55 ; best_test_mse 0.6393715 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 76 [0/10000 (0%)]\tLoss: 0.731210\n",
            "Train epoch: 76 [1280/10000 (37%)]\tLoss: 0.847907\n",
            "Train epoch: 76 [2560/10000 (74%)]\tLoss: 0.660972\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.81057215 0.6393715\n",
            "No improvement since epoch  55 ; best_test_mse 0.6393715 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 77 [0/10000 (0%)]\tLoss: 0.684542\n",
            "Train epoch: 77 [1280/10000 (37%)]\tLoss: 0.691442\n",
            "Train epoch: 77 [2560/10000 (74%)]\tLoss: 0.806830\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.1644788 0.6393715\n",
            "No improvement since epoch  55 ; best_test_mse 0.6393715 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 78 [0/10000 (0%)]\tLoss: 0.757759\n",
            "Train epoch: 78 [1280/10000 (37%)]\tLoss: 0.791852\n",
            "Train epoch: 78 [2560/10000 (74%)]\tLoss: 0.902430\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.0924922 0.6393715\n",
            "No improvement since epoch  55 ; best_test_mse 0.6393715 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 79 [0/10000 (0%)]\tLoss: 0.735898\n",
            "Train epoch: 79 [1280/10000 (37%)]\tLoss: 0.710561\n",
            "Train epoch: 79 [2560/10000 (74%)]\tLoss: 0.770254\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.1931664 0.6393715\n",
            "No improvement since epoch  55 ; best_test_mse 0.6393715 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 80 [0/10000 (0%)]\tLoss: 0.745845\n",
            "Train epoch: 80 [1280/10000 (37%)]\tLoss: 0.938177\n",
            "Train epoch: 80 [2560/10000 (74%)]\tLoss: 0.858488\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.7300357 0.6393715\n",
            "No improvement since epoch  55 ; best_test_mse 0.6393715 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 81 [0/10000 (0%)]\tLoss: 0.752199\n",
            "Train epoch: 81 [1280/10000 (37%)]\tLoss: 0.728808\n",
            "Train epoch: 81 [2560/10000 (74%)]\tLoss: 0.737491\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.0830287 0.6393715\n",
            "No improvement since epoch  55 ; best_test_mse 0.6393715 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 82 [0/10000 (0%)]\tLoss: 0.872615\n",
            "Train epoch: 82 [1280/10000 (37%)]\tLoss: 0.788171\n",
            "Train epoch: 82 [2560/10000 (74%)]\tLoss: 0.913945\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.0300192 0.6393715\n",
            "No improvement since epoch  55 ; best_test_mse 0.6393715 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 83 [0/10000 (0%)]\tLoss: 0.812971\n",
            "Train epoch: 83 [1280/10000 (37%)]\tLoss: 0.772449\n",
            "Train epoch: 83 [2560/10000 (74%)]\tLoss: 0.814453\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.0397092 0.6393715\n",
            "No improvement since epoch  55 ; best_test_mse 0.6393715 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 84 [0/10000 (0%)]\tLoss: 0.727291\n",
            "Train epoch: 84 [1280/10000 (37%)]\tLoss: 0.741810\n",
            "Train epoch: 84 [2560/10000 (74%)]\tLoss: 0.546296\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.9322657 0.6393715\n",
            "No improvement since epoch  55 ; best_test_mse 0.6393715 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 85 [0/10000 (0%)]\tLoss: 0.570352\n",
            "Train epoch: 85 [1280/10000 (37%)]\tLoss: 0.732897\n",
            "Train epoch: 85 [2560/10000 (74%)]\tLoss: 0.761237\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.6200389 0.6393715\n",
            "rmse improved at epoch  85 ; best_test_mse 0.6200389 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 86 [0/10000 (0%)]\tLoss: 0.730570\n",
            "Train epoch: 86 [1280/10000 (37%)]\tLoss: 0.807048\n",
            "Train epoch: 86 [2560/10000 (74%)]\tLoss: 0.810511\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.9474161 0.6200389\n",
            "No improvement since epoch  85 ; best_test_mse 0.6200389 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 87 [0/10000 (0%)]\tLoss: 0.788403\n",
            "Train epoch: 87 [1280/10000 (37%)]\tLoss: 0.725515\n",
            "Train epoch: 87 [2560/10000 (74%)]\tLoss: 0.727463\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.3306592 0.6200389\n",
            "No improvement since epoch  85 ; best_test_mse 0.6200389 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 88 [0/10000 (0%)]\tLoss: 0.919116\n",
            "Train epoch: 88 [1280/10000 (37%)]\tLoss: 0.713164\n",
            "Train epoch: 88 [2560/10000 (74%)]\tLoss: 0.765138\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 1.0970303 0.6200389\n",
            "No improvement since epoch  85 ; best_test_mse 0.6200389 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 89 [0/10000 (0%)]\tLoss: 0.779475\n",
            "Train epoch: 89 [1280/10000 (37%)]\tLoss: 0.710693\n",
            "Train epoch: 89 [2560/10000 (74%)]\tLoss: 0.621178\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.756529 0.6200389\n",
            "No improvement since epoch  85 ; best_test_mse 0.6200389 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 90 [0/10000 (0%)]\tLoss: 0.772798\n",
            "Train epoch: 90 [1280/10000 (37%)]\tLoss: 0.571625\n",
            "Train epoch: 90 [2560/10000 (74%)]\tLoss: 0.723445\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.6820693 0.6200389\n",
            "No improvement since epoch  85 ; best_test_mse 0.6200389 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 91 [0/10000 (0%)]\tLoss: 0.676539\n",
            "Train epoch: 91 [1280/10000 (37%)]\tLoss: 0.776747\n",
            "Train epoch: 91 [2560/10000 (74%)]\tLoss: 0.673932\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.90913993 0.6200389\n",
            "No improvement since epoch  85 ; best_test_mse 0.6200389 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 92 [0/10000 (0%)]\tLoss: 0.866989\n",
            "Train epoch: 92 [1280/10000 (37%)]\tLoss: 0.763321\n",
            "Train epoch: 92 [2560/10000 (74%)]\tLoss: 0.701644\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.6773271 0.6200389\n",
            "No improvement since epoch  85 ; best_test_mse 0.6200389 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 93 [0/10000 (0%)]\tLoss: 0.648424\n",
            "Train epoch: 93 [1280/10000 (37%)]\tLoss: 0.592722\n",
            "Train epoch: 93 [2560/10000 (74%)]\tLoss: 0.817731\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.74553216 0.6200389\n",
            "No improvement since epoch  85 ; best_test_mse 0.6200389 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 94 [0/10000 (0%)]\tLoss: 0.773683\n",
            "Train epoch: 94 [1280/10000 (37%)]\tLoss: 0.684407\n",
            "Train epoch: 94 [2560/10000 (74%)]\tLoss: 0.809504\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.86157066 0.6200389\n",
            "No improvement since epoch  85 ; best_test_mse 0.6200389 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 95 [0/10000 (0%)]\tLoss: 0.751010\n",
            "Train epoch: 95 [1280/10000 (37%)]\tLoss: 0.751489\n",
            "Train epoch: 95 [2560/10000 (74%)]\tLoss: 0.668151\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.75601417 0.6200389\n",
            "No improvement since epoch  85 ; best_test_mse 0.6200389 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 96 [0/10000 (0%)]\tLoss: 0.655128\n",
            "Train epoch: 96 [1280/10000 (37%)]\tLoss: 0.553638\n",
            "Train epoch: 96 [2560/10000 (74%)]\tLoss: 0.695991\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.91487354 0.6200389\n",
            "No improvement since epoch  85 ; best_test_mse 0.6200389 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 97 [0/10000 (0%)]\tLoss: 0.661774\n",
            "Train epoch: 97 [1280/10000 (37%)]\tLoss: 0.661830\n",
            "Train epoch: 97 [2560/10000 (74%)]\tLoss: 0.621648\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.80903506 0.6200389\n",
            "No improvement since epoch  85 ; best_test_mse 0.6200389 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 98 [0/10000 (0%)]\tLoss: 0.576445\n",
            "Train epoch: 98 [1280/10000 (37%)]\tLoss: 0.697708\n",
            "Train epoch: 98 [2560/10000 (74%)]\tLoss: 0.797064\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.6560244 0.6200389\n",
            "No improvement since epoch  85 ; best_test_mse 0.6200389 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 99 [0/10000 (0%)]\tLoss: 0.586613\n",
            "Train epoch: 99 [1280/10000 (37%)]\tLoss: 0.743707\n",
            "Train epoch: 99 [2560/10000 (74%)]\tLoss: 0.744674\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.8733828 0.6200389\n",
            "No improvement since epoch  85 ; best_test_mse 0.6200389 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 100 [0/10000 (0%)]\tLoss: 0.888757\n",
            "Train epoch: 100 [1280/10000 (37%)]\tLoss: 0.788561\n",
            "Train epoch: 100 [2560/10000 (74%)]\tLoss: 0.726931\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.660077 0.6200389\n",
            "No improvement since epoch  85 ; best_test_mse 0.6200389 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 101 [0/10000 (0%)]\tLoss: 0.686947\n",
            "Train epoch: 101 [1280/10000 (37%)]\tLoss: 0.619238\n",
            "Train epoch: 101 [2560/10000 (74%)]\tLoss: 0.688373\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.90234435 0.6200389\n",
            "No improvement since epoch  85 ; best_test_mse 0.6200389 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n",
            "effective drugs,effective prot: 68 442\n",
            "Training on 10000 samples...\n",
            "Train epoch: 102 [0/10000 (0%)]\tLoss: 0.805185\n",
            "Train epoch: 102 [1280/10000 (37%)]\tLoss: 0.783793\n",
            "Train epoch: 102 [2560/10000 (74%)]\tLoss: 0.497588\n",
            "predicting for valid data\n",
            "Make prediction for 5009 samples...\n",
            "valid result: 0.87822664 0.6200389\n",
            "No improvement since epoch  85 ; best_test_mse 0.6200389 GNNNet davis 1\n",
            "train Folds values 20037\n",
            "dataset: davis\n",
            "fold: 1\n",
            "train entries: 20037 effective train entries 10000\n",
            "valid entries: 5009 effective valid entries 5009\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "loaQCmPJef_g"
      },
      "source": [
        "# test "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "48ZaNIvPef_h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "outputId": "f5f29ef0-8f81-4468-ab21-1849f2b600a4"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import numpy as np\n",
        "from random import shuffle\n",
        "import matplotlib.pyplot as plt\n",
        "from torch_geometric.data import Batch\n",
        " \n",
        "# from emetrics import get_aupr, get_cindex, get_rm2, get_ci, get_mse, get_rmse, get_pearson, get_spearman\n",
        "# from utils import *\n",
        "from scipy import stats\n",
        "# from gnn import GNNNet\n",
        "# from data_process import create_dataset_for_test\n",
        " \n",
        " \n",
        "def predicting(model, device, loader):\n",
        "    model.eval()\n",
        "    total_preds = torch.Tensor()\n",
        "    total_labels = torch.Tensor()\n",
        "    print('Make prediction for {} samples...'.format(len(loader.dataset)))\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data_mol_i = data[0].to(device)\n",
        "            data_mol_g = data[1].to(device)\n",
        "            data_pro = data[2].to(device)\n",
        "            # data = data.to(device)\n",
        "            output = model(data_mol_i, data_mol_g, data_pro)\n",
        "            total_preds = torch.cat((total_preds, output.cpu()), 0)\n",
        "            total_labels = torch.cat((total_labels, data_mol_g.y.view(-1, 1).cpu()), 0)\n",
        "    return total_labels.numpy().flatten(), total_preds.numpy().flatten()\n",
        " \n",
        " \n",
        "def load_model(model_path):\n",
        "    model = torch.load(model_path)\n",
        "    return model\n",
        " \n",
        " \n",
        "def calculate_metrics(Y, P, dataset='davis'):\n",
        "    # aupr = get_aupr(Y, P)\n",
        "    cindex = get_cindex(Y, P)  # DeepDTA\n",
        "    cindex2 = get_ci(Y, P)  # GraphDTA\n",
        "    rm2 = get_rm2(Y, P)  # DeepDTA\n",
        "    mse = get_mse(Y, P)\n",
        "    pearson = get_pearson(Y, P)\n",
        "    spearman = get_spearman(Y, P)\n",
        "    rmse = get_rmse(Y, P)\n",
        " \n",
        "    print('metrics for ', dataset)\n",
        "    # print('aupr:', aupr)\n",
        "    print('cindex:', cindex)\n",
        "    print('cindex2', cindex2)\n",
        "    print('rm2:', rm2)\n",
        "    print('mse:', mse)\n",
        "    print('pearson', pearson)\n",
        " \n",
        "    result_file_name = '/content/gdrive/My Drive/DL_Project_local/results/NewModel_Image_GCN' + model_st + '_' + dataset + '.txt'\n",
        "    result_str = ''\n",
        "    result_str += dataset + '\\r\\n'\n",
        "    result_str += 'rmse:' + str(rmse) + ' ' + ' mse:' + str(mse) + ' ' + ' pearson:' + str(\n",
        "        pearson) + ' ' + 'spearman:' + str(spearman) + ' ' + 'ci:' + str(cindex) + ' ' + 'rm2:' + str(rm2)\n",
        "    print(result_str)\n",
        "    open(result_file_name, 'w').writelines(result_str)\n",
        " \n",
        " \n",
        "def plot_density(Y, P, fold=0, dataset='davis'):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.grid(linestyle='--')\n",
        "    ax = plt.gca()\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.spines['right'].set_visible(False)\n",
        " \n",
        "    plt.scatter(P, Y, color='blue', s=40)\n",
        "    plt.title('density of ' + dataset, fontsize=30, fontweight='bold')\n",
        "    plt.xlabel('predicted', fontsize=30, fontweight='bold')\n",
        "    plt.ylabel('measured', fontsize=30, fontweight='bold')\n",
        "    # plt.xlim(0, 21)\n",
        "    # plt.ylim(0, 21)\n",
        "    if dataset == 'davis':\n",
        "        plt.plot([5, 11], [5, 11], color='black')\n",
        "    else:\n",
        "        plt.plot([6, 16], [6, 16], color='black')\n",
        "    # plt.legend()\n",
        "    plt.legend(loc=0, numpoints=1)\n",
        "    leg = plt.gca().get_legend()\n",
        "    ltext = leg.get_texts()\n",
        "    plt.setp(ltext, fontsize=12, fontweight='bold')\n",
        "    plt.savefig(os.path.join('/content/gdrive/My Drive/DL_Project_local/results', 'NewModel_Image_GCN' + dataset + '_' + str(fold) + '.png'), dpi=500, bbox_inches='tight')\n",
        " \n",
        " \n",
        "if __name__ == '__main__':\n",
        "    dataset = 'davis'  # dataset selection\n",
        "    model_st = GNNNet.__name__\n",
        "    print('dataset:', dataset)\n",
        " \n",
        "    cuda_name = 'cpu'#'cuda:0'  # gpu selection\n",
        "    print('cuda_name:', cuda_name)\n",
        " \n",
        "    TEST_BATCH_SIZE = 384\n",
        "    fold = 1\n",
        "    models_dir = 'models'\n",
        "    results_dir = 'results'\n",
        " \n",
        "    device = torch.device(cuda_name if torch.cuda.is_available() else 'cpu')\n",
        "    model_file_name = '/root/DGraphDTA/models/model_' + model_st + '_' + dataset + '.model'\n",
        "    result_file_name = '/root/DGraphDTA/results/result_' + model_st + '_' + dataset + '.txt'\n",
        "    \n",
        " \n",
        "    model = GNNNet()\n",
        "    model.to(device)\n",
        "    dummy = torch.load('/content/gdrive/My Drive/DL_Project_local/models/model_GNNNet_CNN_davis_1_0218_282_228epochs_100Image_newModel_p3.model', map_location=cuda_name)\n",
        "    model.load_state_dict(dummy['state_dict'])\n",
        "    \n",
        "    #model.load_state_dict(torch.load(model_file_name, map_location=cuda_name))\n",
        "    \n",
        "    test_data = create_dataset_for_test(dataset)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=TEST_BATCH_SIZE, shuffle=False,\n",
        "                                              collate_fn=collate)\n",
        "    #test_data, dum = create_dataset_for_5folds(dataset, fold)\n",
        "    #test_loader = torch.utils.data.DataLoader(test_data, batch_size=TEST_BATCH_SIZE, shuffle=False,\n",
        "    #                                          collate_fn=collate)\n",
        "    #Y, P = predicting(model, device, test_loader)\n",
        "    #vv = pd.DataFrame(P, Y)\n",
        "    #vv.to_csv('/content/gdrive/My Drive/DL_Project/Drug_Images_generated_BA_Train.csv')\n",
        "    #test_loader = torch.utils.data.DataLoader(dum, batch_size=TEST_BATCH_SIZE, shuffle=False,\n",
        "    #                                          collate_fn=collate)\n",
        "    Y, P = predicting(model, device, test_loader)\n",
        "   # vv_v = pd.DataFrame(P, Y)\n",
        "   # vv_v.to_csv('/content/gdrive/My Drive/DL_Project/generated_BA_Valid.csv')\n",
        "    calculate_metrics(Y, P, dataset)\n",
        "    plot_density(Y, P,0, dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset: davis\n",
            "cuda_name: cpu\n",
            "GNNNet Loaded\n",
            "testfold rows negth 5010\n",
            "testfold cols negth 5010\n",
            "dataset: davis\n",
            "test entries: 5010 effective test entries 5010\n",
            "effective drugs,effective prot: 68 442\n",
            "Make prediction for 5010 samples...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "No handles with labels found to put in legend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "metrics for  davis\n",
            "cindex: 0.7712995257328518\n",
            "cindex2 0.7743490131532095\n",
            "rm2: 0.28761782003327374\n",
            "mse: 0.5614629\n",
            "pearson 0.5893391653329082\n",
            "davis\r\n",
            "rmse:0.7493082671243579  mse:0.5614629  pearson:0.5893391653329082 spearman:0.5063924579521937 ci:0.7712995257328518 rm2:0.28761782003327374\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}